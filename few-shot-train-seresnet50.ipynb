{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fbb1f7-1ac7-4293-8761-b388f2ef5fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_addons as tfa\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, precision_score, recall_score, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397dfa5-eb65-419a-8044-4ddbac706329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, MaxPool2D, \\\n",
    "GlobalAveragePooling2D, Conv2DTranspose, Concatenate, Input, Dense, Reshape, Multiply, add, Flatten, ZeroPadding2D\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.utils.layer_utils import get_source_inputs\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc299db8-c95f-470e-89ae-2f41a6cb6301",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "ORI_SIZE = (271, 481)\n",
    "IMG_H = 128\n",
    "IMG_W = 128\n",
    "IMG_C = 3  ## Change this to 1 for grayscale.\n",
    "winSize = (256, 256)\n",
    "stSize = 20\n",
    "# Weight initializers for the Generator network\n",
    "# WEIGHT_INIT = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.2)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "TRAIN = True\n",
    "\n",
    "LIMIT_EVAL_IMAGES = 100\n",
    "LIMIT_TEST_IMAGES = \"MAX\"\n",
    "LIMIT_TRAIN_IMAGES = \"MAX\"\n",
    "\n",
    "# range between 0-1\n",
    "anomaly_weight = 0.7\n",
    "learning_rate = 0.002\n",
    "meta_step_size = 0.25\n",
    "\n",
    "inner_batch_size = 25\n",
    "eval_batch_size = 25\n",
    "\n",
    "meta_iters = 2000\n",
    "eval_iters = 1\n",
    "inner_iters = 4\n",
    "\n",
    "train_shots = 40\n",
    "shots = 20\n",
    "classes = 1\n",
    "\n",
    "dataset_name = \"mura\"\n",
    "eval_dataset_name = \"mura\"\n",
    "test_dataset_name = \"mura\"\n",
    "\n",
    "mode_colour = str(IMG_H) + \"_rgb\"\n",
    "if IMG_C == 1:\n",
    "    mode_colour = str(IMG_H) + \"_gray\"\n",
    "    \n",
    "name_model = mode_colour+\"_\"+dataset_name+\"_few_shot_anomaly_detection\"+\"_\"+str(meta_iters)\n",
    "g_model_path = \"saved_model/\"+name_model+\"_g_model.h5\"\n",
    "d_model_path = \"saved_model/\"+name_model+\"_d_model.h5\"\n",
    "\n",
    "\n",
    "train_data_path = \"data/\"+dataset_name+\"/train_data\"\n",
    "eval_data_path = \"data/\"+eval_dataset_name+\"/eval_data\"\n",
    "test_data_path = \"data/\"+test_dataset_name+\"/test_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828014f0-4825-4f9c-8e4e-c50bff9d4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for SSIM loss function\n",
    "class SSIMLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self,\n",
    "         reduction=tf.keras.losses.Reduction.AUTO,\n",
    "         name='SSIMLoss'):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "\n",
    "    def call(self, ori, recon):\n",
    "        recon = tf.convert_to_tensor(recon)\n",
    "        ori = tf.cast(ori, recon.dtype)\n",
    "\n",
    "        # Loss 3: SSIM Loss\n",
    "#         loss_ssim =  tf.reduce_mean(1 - tf.image.ssim(ori, recon, max_val=1.0)[0]) \n",
    "        loss_ssim = tf.reduce_mean(1 - tf.image.ssim(ori, recon, max_val=IMG_W, filter_size=7, k1=0.01 ** 2, k2=0.03 ** 2))\n",
    "        return loss_ssim\n",
    "    \n",
    "\n",
    "class MultiFeatureLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self,\n",
    "             reduction=tf.keras.losses.Reduction.AUTO,\n",
    "             name='FeatureLoss'):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.mse_func = tf.keras.losses.MeanSquaredError() \n",
    "\n",
    "    \n",
    "    def call(self, real, fake, weight=1):\n",
    "        result = 0.0\n",
    "        for r, f in zip(real, fake):\n",
    "            result = result + (weight * self.mse_func(r, f))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    \n",
    "# class for Adversarial loss function\n",
    "class AdversarialLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self,\n",
    "             reduction=tf.keras.losses.Reduction.AUTO,\n",
    "             name='AdversarialLoss'):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "\n",
    "    \n",
    "    def call(self, logits_in, labels_in):\n",
    "        labels_in = tf.convert_to_tensor(labels_in)\n",
    "        logits_in = tf.cast(logits_in, labels_in.dtype)\n",
    "        # Loss 4: FEATURE Loss\n",
    "        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_in, labels=labels_in))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f0129-2529-4ace-bbef-848d8c4f8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' calculate the auc value for lables and scores'''\n",
    "def roc(labels, scores, name_model):\n",
    "    \"\"\"Compute ROC curve and ROC area for each class\"\"\"\n",
    "    roc_auc = dict()\n",
    "    # True/False Positive Rates.\n",
    "    fpr, tpr, threshold = roc_curve(labels, scores)\n",
    "    # print(\"threshold: \", threshold)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # get a threshod that perform very well.\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = threshold[optimal_idx]\n",
    "    # draw plot for ROC-Curve\n",
    "    # plot_roc_curve(fpr, tpr, name_model)\n",
    "    \n",
    "    return roc_auc, optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c231433-0c03-4a84-a2b5-d40b36825ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delcare all loss function that we will use\n",
    "# L1 Loss\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "# L2 Loss\n",
    "mse = tf.keras.losses.MeanSquaredError() \n",
    "\n",
    "multimse = MultiFeatureLoss()\n",
    "# SSIM loss\n",
    "ssim = SSIMLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229876d-50de-4906-ba58-519932cfec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCAdam(tf.keras.optimizers.Adam):\n",
    "    def get_gradients(self, loss, params):\n",
    "        # We here just provide a modified get_gradients() function since we are\n",
    "        # trying to just compute the centralized gradients.\n",
    "\n",
    "        grads = []\n",
    "        gradients = super().get_gradients()\n",
    "        for grad in gradients:\n",
    "            grad_len = len(grad.shape)\n",
    "            if grad_len > 1:\n",
    "                axis = list(range(grad_len - 1))\n",
    "                grad -= tf.reduce_mean(grad, axis=axis, keep_dims=True)\n",
    "            grads.append(grad)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47377645-0e75-4ee5-881c-99f25b96b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(examples, epoch, n):\n",
    "    examples = (examples + 1) / 2.0\n",
    "    for i in range(n * n):\n",
    "        plt.subplot(n, n, i+1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(examples[i])  ## pyplot.imshow(np.squeeze(examples[i], axis=-1))\n",
    "    filename = f\"samples/generated_plot_epoch-{epoch}.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f84aa3-8208-4a3c-b433-d5a2d1791301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(title+'_cm.png')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa049b05-f27c-45be-949a-c362baa633dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epoch_result(iters, loss, name, model_name, colour):\n",
    "    plt.plot(iters, loss, colour, label=name)\n",
    "#     plt.plot(epochs, disc_loss, 'b', label='Discriminator loss')\n",
    "    plt.title(name)\n",
    "    plt.xlabel('Iters')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(model_name+ '_'+name+'_iters_result.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "def enhance_image(image, beta=0.1):\n",
    "    image = tf.cast(image, tf.float64)\n",
    "    image = ((1 + beta) * image) + (-beta * tf.math.reduce_mean(image))\n",
    "    return image\n",
    "\n",
    "def crop_left_and_right(img):\n",
    "    # img_shape = tf.shape(img)\n",
    "    img_left = tf.image.crop_to_bounding_box(img, 0, 0, IMG_H, IMG_W)\n",
    "    img_right = tf.image.crop_to_bounding_box(img, ORI_SIZE[0] - IMG_H, ORI_SIZE[1] - IMG_W, IMG_H, IMG_W)\n",
    "    \n",
    "    return [img_left, img_right]\n",
    "\n",
    "def crop_left_and_right_select_one(img):\n",
    "    # img_shape = tf.shape(img)\n",
    "    img_left = tf.image.crop_to_bounding_box(img, 0, 0, IMG_H, IMG_W)\n",
    "    img_right = tf.image.crop_to_bounding_box(img, ORI_SIZE[0] - IMG_H, ORI_SIZE[1] - IMG_W, IMG_H, IMG_W)\n",
    "    if tf.math.reduce_std(img_left) <  tf.math.reduce_std(img_right):\n",
    "        return img_left\n",
    "    return img_right\n",
    "\n",
    "def sliding_crop_and_select_one(img, stepSize=stSize, windowSize=winSize):\n",
    "    current_std = 0\n",
    "    current_image = None\n",
    "    y_end_crop, x_end_crop = False, False\n",
    "    for y in range(0, ORI_SIZE[0], stepSize):\n",
    "        \n",
    "        y_end_crop = False\n",
    "        \n",
    "        for x in range(0, ORI_SIZE[1], stepSize):\n",
    "            \n",
    "            x_end_crop = False\n",
    "            \n",
    "            crop_y = y\n",
    "            if (y + windowSize[0]) > ORI_SIZE[0]:\n",
    "                crop_y =  ORI_SIZE[0] - windowSize[0]\n",
    "                y_end_crop = True\n",
    "            \n",
    "            crop_x = x\n",
    "            if (x + windowSize[1]) > ORI_SIZE[1]:\n",
    "                crop_x = ORI_SIZE[1] - windowSize[1]\n",
    "                x_end_crop = True\n",
    "                \n",
    "            image = tf.image.crop_to_bounding_box(img, crop_y, crop_x, windowSize[0], windowSize[1])                \n",
    "            std_image = tf.math.reduce_std(tf.cast(image, dtype=tf.float32))\n",
    "          \n",
    "            if current_std == 0 or std_image < current_std :\n",
    "                current_std = std_image\n",
    "                current_image = image\n",
    "                \n",
    "            if x_end_crop:\n",
    "                break\n",
    "                \n",
    "        if x_end_crop and y_end_crop:\n",
    "            break\n",
    "            \n",
    "    return current_image\n",
    "\n",
    "def sliding_crop(img, stepSize=stSize, windowSize=winSize):\n",
    "    current_std = 0\n",
    "    current_image = []\n",
    "    y_end_crop, x_end_crop = False, False\n",
    "    for y in range(0, ORI_SIZE[0], stepSize):\n",
    "        y_end_crop = False\n",
    "        for x in range(0, ORI_SIZE[1], stepSize):\n",
    "            x_end_crop = False\n",
    "            crop_y = y\n",
    "            if (y + windowSize[0]) > ORI_SIZE[0]:\n",
    "                crop_y =  ORI_SIZE[0] - windowSize[0]\n",
    "            \n",
    "            crop_x = x\n",
    "            if (x + windowSize[1]) > ORI_SIZE[1]:\n",
    "                crop_x = ORI_SIZE[1] - windowSize[1]\n",
    "            \n",
    "            # print(crop_y, crop_x, windowSize)\n",
    "            image = tf.image.crop_to_bounding_box(img, crop_y, crop_x, windowSize[0], windowSize[1])\n",
    "            current_image.append(image)\n",
    "            if x_end_crop:\n",
    "                break\n",
    "        if x_end_crop and y_end_crop:\n",
    "            break\n",
    "    return current_image\n",
    "\n",
    "def custom_v3(img):\n",
    "    img = tf.image.adjust_gamma(img)\n",
    "    img = tfa.image.median_filter2d(img, 3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80474f87-23e7-4760-9a05-1d76ca4d740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_with_labels(filepath, class_names, training, limit=100):\n",
    "   \n",
    "    image_list = []\n",
    "    label_list = []\n",
    "    for class_n in class_names:  # do dogs and cats\n",
    "        path = os.path.join(filepath,class_n)  # create path to dogs and cats\n",
    "        class_num = class_names.index(class_n)  # get the classification  (0 or a 1). 0=dog 1=cat\n",
    "        path_list = []\n",
    "        class_list = []\n",
    "        for img in tqdm(os.listdir(path)):  \n",
    "            if \".DS_Store\" != img:\n",
    "                filpath = os.path.join(path,img)\n",
    "#                 print(filpath, class_num)\n",
    "                \n",
    "                path_list.append(filpath)\n",
    "                class_list.append(class_num)\n",
    "                # image_label_list.append({filpath:class_num})\n",
    "        \n",
    "        n_samples = None\n",
    "        if limit != \"MAX\":\n",
    "            n_samples = limit\n",
    "                    \n",
    "        path_list, class_list = shuffle(path_list, class_list, n_samples=n_samples ,random_state=random.randint(123, 10000))\n",
    "        \n",
    "        image_list = image_list + path_list\n",
    "        label_list = label_list + class_list\n",
    "  \n",
    "    # print(image_list, label_list)\n",
    "    \n",
    "    return image_list, label_list\n",
    "\n",
    "def prep_stage(x, train=True):\n",
    "    beta_contrast = 0.1\n",
    "    \n",
    "    if train:\n",
    "        # x = enhance_image(x, beta_contrast)\n",
    "        x = tfa.image.equalize(x)\n",
    "        # x = custom_v3(x)\n",
    "    else: \n",
    "        # x = enhance_image(x, beta_contrast)\n",
    "        x = tfa.image.equalize(x)\n",
    "        # x = custom_v3(x)\n",
    "    return x\n",
    "\n",
    "def post_stage(x):\n",
    "    \n",
    "    x = tf.image.resize(x, (IMG_H, IMG_W))\n",
    "    # x = tf.image.resize_with_crop_or_pad(x, IMG_H, IMG_W)\n",
    "    # normalize to the range -1,1\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    x = (x - 127.5) / 127.5\n",
    "    # normalize to the range 0-1\n",
    "    # img /= 255.0\n",
    "    return x\n",
    "\n",
    "def extraction(image, label):\n",
    "    # This function will shrink the Omniglot images to the desired size,\n",
    "    # scale pixel values and convert the RGB image to grayscale\n",
    "    img = tf.io.read_file(image)\n",
    "    img = tf.io.decode_png(img, channels=IMG_C)\n",
    "    # img = tf.io.decode_bmp(img, channels=IMG_C)\n",
    "    img = prep_stage(img, True)\n",
    "    # img = crop_left_and_right_select_one(img)\n",
    "    img = sliding_crop_and_select_one(img)\n",
    "    img = post_stage(img)\n",
    "\n",
    "    return img, label\n",
    "\n",
    "def extraction_test(image, label):\n",
    "    # This function will shrink the Omniglot images to the desired size,\n",
    "    # scale pixel values and convert the RGB image to grayscale\n",
    "    img = tf.io.read_file(image)\n",
    "    img = tf.io.decode_png(img, channels=IMG_C)\n",
    "    # img = tf.io.decode_bmp(img, channels=IMG_C)\n",
    "    img = prep_stage(img, False)\n",
    "    # img = post_stage(img)\n",
    "    \n",
    "    \n",
    "    # img_list = crop_left_and_right(img)\n",
    "    img_list = sliding_crop(img)\n",
    "    \n",
    "    img = [post_stage(a) for a in img_list]\n",
    "\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd40c6-f382-409e-822e-9fa683bf0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_gen_disc(mode, g_model_inner, d_model_inner, g_filepath, d_filepath, test_data_path):\n",
    "    print(\"Start Checking Reconstructed Image\")\n",
    "    g_model_inner.load_weights(g_filepath)\n",
    "    d_model_inner.load_weights(d_filepath)\n",
    "    paths = {\n",
    "        \"normal\": test_data_path+\"/normal/normal.png\",\n",
    "        \"defect\": test_data_path+\"/defect/defect.png\",\n",
    "    }\n",
    "\n",
    "    for i, v in paths.items():\n",
    "        print(i,v)\n",
    "\n",
    "        width=IMG_W\n",
    "        height=IMG_H\n",
    "        rows = 1\n",
    "        cols = 3\n",
    "        axes=[]\n",
    "        fig = plt.figure()\n",
    "\n",
    "        \n",
    "        img, label = extraction(v, i)\n",
    "       \n",
    "        name_subplot = mode+'_original_'+i\n",
    "        axes.append( fig.add_subplot(rows, cols, 1) )\n",
    "        axes[-1].set_title('_original_')  \n",
    "        plt.imshow(img.numpy().astype(\"int64\"), alpha=1.0)\n",
    "        plt.axis('off')\n",
    "\n",
    "       \n",
    "        img = tf.cast(img, tf.float64)\n",
    "        img = (img - 127.5) / 127.5\n",
    "\n",
    "\n",
    "        image = tf.reshape(img, (-1, IMG_H, IMG_W, IMG_C))\n",
    "        reconstructed_images = g_model_inner.predict(image)\n",
    "        reconstructed_images = tf.reshape(reconstructed_images, (IMG_H, IMG_W, IMG_C))\n",
    "        reconstructed_images = reconstructed_images * 127 + 127\n",
    "\n",
    "        name_subplot = mode+'_reconstructed_'+i\n",
    "        axes.append( fig.add_subplot(rows, cols, 3) )\n",
    "        axes[-1].set_title('_reconstructed_') \n",
    "        plt.imshow(reconstructed_images.numpy().astype(\"int64\"), alpha=1.0)\n",
    "        plt.axis('off')\n",
    "\n",
    "        fig.tight_layout()    \n",
    "        fig.savefig(mode+'_'+i+'.png')\n",
    "        plt.show()\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241a868-2a33-42de-b550-605c148710c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    # This class will facilitate the creation of a few-shot dataset\n",
    "    # from the Omniglot dataset that can be sampled from quickly while also\n",
    "    # allowing to create new labels at the same time.\n",
    "    def __init__(self, path_file, training=True, limit=100):\n",
    "        # Download the tfrecord files containing the omniglot data and convert to a\n",
    "        # dataset.\n",
    "        self.data = {}\n",
    "        \n",
    "        class_names = [\"normal\"] if training else [\"normal\", \"defect\"]\n",
    "        filenames, labels = read_data_with_labels(path_file, class_names, training, limit)\n",
    "        \n",
    "        ds = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "        self.ds = ds.shuffle(buffer_size=1024, seed=random.randint(123, 10000) )\n",
    "        \n",
    "        \n",
    "        if training:\n",
    "            for image, label in ds.map(extraction):\n",
    "                image = image.numpy()\n",
    "                label = str(label.numpy())\n",
    "                if label not in self.data:\n",
    "                    self.data[label] = []\n",
    "                self.data[label].append(image)\n",
    "            self.labels = list(self.data.keys())\n",
    "            \n",
    "\n",
    "    def get_mini_dataset(\n",
    "        self, batch_size, repetitions, shots, num_classes, split=False\n",
    "    ):\n",
    "        temp_labels = np.zeros(shape=(num_classes * shots))\n",
    "        temp_images = np.zeros(shape=(num_classes * shots, IMG_H, IMG_W, IMG_C))\n",
    "        if split:\n",
    "            test_labels = np.zeros(shape=(num_classes))\n",
    "            test_images = np.zeros(shape=(num_classes, IMG_H, IMG_W, IMG_C))\n",
    "\n",
    "        # Get a random subset of labels from the entire label set.\n",
    "        label_subset = random.choices(self.labels, k=num_classes)\n",
    "        for class_idx, class_obj in enumerate(label_subset):\n",
    "            # Use enumerated index value as a temporary label for mini-batch in\n",
    "            # few shot learning.\n",
    "            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\n",
    "            # If creating a split dataset for testing, select an extra sample from each\n",
    "            # label to create the test dataset.\n",
    "            if split:\n",
    "                test_labels[class_idx] = class_idx\n",
    "                images_to_split = random.choices(\n",
    "                    self.data[label_subset[class_idx]], k=shots + 1\n",
    "                )\n",
    "                test_images[class_idx] = images_to_split[-1]\n",
    "                temp_images[\n",
    "                    class_idx * shots : (class_idx + 1) * shots\n",
    "                ] = images_to_split[:-1]\n",
    "            else:\n",
    "                # For each index in the randomly selected label_subset, sample the\n",
    "                # necessary number of images.\n",
    "                temp_images[\n",
    "                    class_idx * shots : (class_idx + 1) * shots\n",
    "                ] = random.choices(self.data[label_subset[class_idx]], k=shots)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (temp_images.astype(np.float32), temp_labels.astype(np.int32))\n",
    "        )\n",
    "        dataset = dataset.shuffle(100).batch(batch_size).repeat(repetitions)\n",
    "        \n",
    "        if split:\n",
    "            return dataset, test_images, test_labels\n",
    "        return dataset\n",
    "    \n",
    "    def get_dataset(self, batch_size):\n",
    "        ds = self.ds.map(extraction_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        ds = ds.batch(batch_size)\n",
    "        ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings() # Disable SSL warnings that may happen during download.\n",
    "\n",
    "## load dataset\n",
    "train_dataset = Dataset(train_data_path, training=True, limit=LIMIT_TRAIN_IMAGES)\n",
    "\n",
    "eval_dataset = Dataset(eval_data_path, training=False, limit=LIMIT_EVAL_IMAGES)\n",
    "eval_ds = eval_dataset.get_dataset(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeff0229-24d5-497f-9aad-ecc78421b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, axarr = plt.subplots(nrows=2, ncols=5, figsize=(20, 20))\n",
    "# sample_keys = list(test_dataset.data.keys())\n",
    "# # print(sample_keys)\n",
    "# for a in range(2):\n",
    "#     for b in range(5):\n",
    "#         temp_image = test_dataset.data[sample_keys[a]][b]\n",
    "#         temp_image = np.stack((temp_image[:, :, 0],) * 3, axis=2)\n",
    "#         temp_image *= 255\n",
    "#         temp_image = np.clip(temp_image, 0, 255).astype(\"uint8\")\n",
    "#         if b == 2:\n",
    "#             axarr[a, b].set_title(\"Class : \" + sample_keys[a])\n",
    "#         axarr[a, b].imshow(temp_image)\n",
    "#         axarr[a, b].xaxis.set_visible(False)\n",
    "#         axarr[a, b].yaxis.set_visible(False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19900ec8-7ac3-441e-b030-c0e7cda96824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_a_score(out_g_model, out_d_model, images):\n",
    "    reconstructed_images = out_g_model(images, training=False)\n",
    "\n",
    "    feature_real, label_real  = out_d_model(images, training=False)\n",
    "    # print(generated_images.shape)\n",
    "    feature_fake, label_fake = out_d_model(reconstructed_images, training=False)\n",
    "\n",
    "    # Loss 2: RECONSTRUCTION loss (L1)\n",
    "    loss_rec = mae(images, reconstructed_images)\n",
    "\n",
    "    loss_feat = multimse(feature_real, feature_fake)\n",
    "    # print(\"loss_rec:\", loss_rec, \"loss_feat:\", loss_feat)\n",
    "    score = (anomaly_weight * loss_rec) + ((1-anomaly_weight) * loss_feat)\n",
    "    return score, loss_rec, loss_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9da365-5389-47a1-aadc-18bc0ba50f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block_2nd(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block_2nd(x, num_filters)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05eaa2a-cae0-418b-b9b4-5dc879ebcaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    bn_eps = 0.0001\n",
    "        \n",
    "    block_name = str(stage) + \"_\" + str(block)\n",
    "    conv_name_base = \"conv\" + block_name\n",
    "    relu_name_base = \"relu\" + block_name\n",
    "\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), use_bias=False, name=conv_name_base + '_x1')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=bn_eps, name=conv_name_base + '_x1_bn')(x)\n",
    "    x = Activation('relu', name=relu_name_base + '_x1')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, padding='same', use_bias=False, name=conv_name_base + '_x2')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=bn_eps, name=conv_name_base + '_x2_bn')(x)\n",
    "    x = Activation('relu', name=relu_name_base + '_x2')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), use_bias=False, name=conv_name_base + '_x3')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=bn_eps, name=conv_name_base + '_x3_bn')(x)\n",
    "\n",
    "    se = GlobalAveragePooling2D(name='pool' + block_name + '_gap')(x)\n",
    "    se = Dense(filters3 // 16, activation='relu', name = 'fc' + block_name + '_sqz')(se)\n",
    "    se = Dense(filters3, activation='sigmoid', name = 'fc' + block_name + '_exc')(se)\n",
    "    se = Reshape([1, 1, filters3])(se)\n",
    "    x = Multiply(name='scale' + block_name)([x, se])\n",
    "\n",
    "    x = add([x, input_tensor], name='block_' + block_name + '_x4')\n",
    "    x = Activation('relu', name='block_out_' + block_name + '_x4')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    bn_eps = 0.0001\n",
    "    \n",
    "    block_name = str(stage) + \"_\" + str(block)\n",
    "    conv_name_base = \"conv\" + block_name\n",
    "    relu_name_base = \"relu\" + block_name\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), use_bias=False, name=conv_name_base + '_x1')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=bn_eps, name=conv_name_base + '_x1_bn')(x)\n",
    "    x = Activation('relu', name=relu_name_base + '_x1')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, strides=strides, padding='same', use_bias=False, name=conv_name_base + '_x2')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=bn_eps, name=conv_name_base + '_x2_bn')(x)\n",
    "    x = Activation('relu', name=relu_name_base + '_x2')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), use_bias=False, name=conv_name_base + '_x3')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=bn_eps, name=conv_name_base + '_x3_bn')(x)\n",
    "    \n",
    "    se = GlobalAveragePooling2D(name='pool' + block_name + '_gap')(x)\n",
    "    se = Dense(filters3 // 16, activation='relu', name = 'fc' + block_name + '_sqz')(se)\n",
    "    se = Dense(filters3, activation='sigmoid', name = 'fc' + block_name + '_exc')(se)\n",
    "    se = Reshape([1, 1, filters3])(se)\n",
    "    x = Multiply(name='scale' + block_name)([x, se])\n",
    "    \n",
    "    shortcut = Conv2D(filters3, (1, 1), strides=strides, use_bias=False, name=conv_name_base + '_prj')(input_tensor)\n",
    "    shortcut = BatchNormalization(axis=bn_axis, epsilon=bn_eps, name=conv_name_base + '_prj_bn')(shortcut)\n",
    "\n",
    "    x = add([x, shortcut], name='block_' + block_name)\n",
    "    x = Activation('relu', name='block_out_' + block_name)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def SEResNet50(include_top=True, weights='imagenet',\n",
    "               input_tensor=None, input_shape=None,\n",
    "               pooling=None,\n",
    "               classes=1000):\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=225,\n",
    "                                      min_size=160,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    bn_eps = 0.0001\n",
    "    \n",
    "    # x = ZeroPadding2D(padding=(2, 2), name='conv1_pad')(img_input)\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), padding='same', use_bias=False, name='conv1')(img_input)\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=bn_eps, name='conv1_bn')(x)\n",
    "    x = Activation('relu', name='conv1_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='conv1_pool')(x)\n",
    "    # x = ZeroPadding2D(padding=(1, 1), name='conv1_pad')(x)\n",
    "    \n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block=1, strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block=2)\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block=3)\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block=1)\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block=2)\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block=3)\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block=4)\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block=1)\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=2)\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=3)\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=4)\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=5)\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=6)\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block=1)\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block=2)\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block=3)\n",
    "\n",
    "    # x = Flatten()(x)\n",
    "    # x = Dense(classes, activation='softmax', name='fc6')(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='se-resnet50')\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426bae8-7541-4974-9364-c3a05098faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seresnet50_unet(input_shape):\n",
    "    inputs = Input(input_shape, name=\"input_1\")\n",
    "    \"\"\" Pre-trained ResNet50 Model \"\"\"\n",
    "    seresnet50 = SEResNet50(weights=None, input_tensor=inputs)\n",
    "    seresnet50.summary()\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    s1 = seresnet50.get_layer(\"input_1\").output           ## (512 x 512)\n",
    "    s2 = seresnet50.get_layer(\"conv1_relu\").output        ## (256 x 256)\n",
    "    s3 = seresnet50.get_layer(\"relu3_1_x1\").output  ## (128 x 128)\n",
    "    s4 = seresnet50.get_layer(\"relu4_1_x1\").output  ## (64 x 64)\n",
    "    s5 = seresnet50.get_layer(\"relu5_1_x1\").output  ## (32 x 32)\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    b1 = seresnet50.get_layer(\"block_out_5_3_x4\").output  ## (16 x 16)\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    x = IMG_SIZE\n",
    "    d1 = decoder_block(b1, s5, x)                     ## (32 x 32)\n",
    "    x = x/2\n",
    "    d2 = decoder_block(d1, s4, x)                     ## (64 x 64)\n",
    "    x = x/2\n",
    "    d3 = decoder_block(d2, s3, x)                     ## (128 x 128)\n",
    "    x = x/2\n",
    "    d4 = decoder_block(d3, s2, x)                      ## (256 x 256)\n",
    "    x = x/2\n",
    "    d5 = decoder_block(d4, s1, x)                      ## (512 x 512)\n",
    "    \n",
    "    \"\"\" Output \"\"\"\n",
    "    outputs = tf.keras.layers.Conv2D(IMG_C, 1, padding=\"same\", activation=\"tanh\")(d5)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9aad2-6f9e-40b4-94ab-3c965a4a6f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create discriminator model\n",
    "def build_discriminator(inputs):\n",
    "    num_layers = 4\n",
    "    if IMG_H > 128:\n",
    "        num_layers = 5\n",
    "    f = [2**i for i in range(num_layers)]\n",
    "    x = inputs\n",
    "    features = []\n",
    "    for i in range(0, num_layers):\n",
    "        if i == 0:\n",
    "            x = tf.keras.layers.DepthwiseConv2D(kernel_size = (3, 3), strides=(2, 2), padding='same')(x)\n",
    "            x = tf.keras.layers.Conv2D(f[i] * IMG_H ,kernel_size = (1, 1),strides=(2,2), padding='same')(x)\n",
    "            # x = tf.keras.layers.BatchNormalization()(x)\n",
    "            x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        else:\n",
    "            x = tf.keras.layers.DepthwiseConv2D(kernel_size = (3, 3), strides=(2, 2), padding='same')(x)\n",
    "            x = tf.keras.layers.Conv2D(f[i] * IMG_H ,kernel_size = (1, 1),strides=(2,2), padding='same')(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "        # x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        \n",
    "        features.append(x)\n",
    "           \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    features.append(x)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs, outputs = [features, output])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9333e-3ce6-4477-b5f3-e0e377a3fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(g_model_inner, d_model_inner, g_filepath, d_filepath, test_ds):\n",
    "    class_names = [\"normal\", \"defect\"] # normal = 0, defect = 1\n",
    "    \n",
    "    g_model_inner.load_weights(g_filepath)\n",
    "    d_model_inner.load_weights(d_filepath)\n",
    "    \n",
    "        \n",
    "    scores_ano = []\n",
    "    real_label = []\n",
    "    rec_loss_list = []\n",
    "    feat_loss_list = []\n",
    "    ssim_loss_list = []\n",
    "    counter = 0\n",
    "    for images, labels in test_ds:\n",
    "        loss_rec, loss_feat = 0.0, 0.0\n",
    "        score = 0\n",
    "        \n",
    "        counter += 1\n",
    "        '''for normal'''\n",
    "        # temp_score, loss_rec, loss_feat = calculate_a_score(g_model_inner, d_model_inner, images)\n",
    "        # score = temp_score.numpy()\n",
    "        \n",
    "        \n",
    "        '''for sliding images & Crop LR'''\n",
    "        for image in images:\n",
    "            r_score, r_rec_loss, r_feat_loss = calculate_a_score(g_model_inner, d_model_inner, image)\n",
    "            if r_score.numpy() > score or score == 0:\n",
    "                score = r_score.numpy()\n",
    "                loss_rec = r_rec_loss\n",
    "                loss_feat = r_feat_loss\n",
    "                \n",
    "            \n",
    "        scores_ano = np.append(scores_ano, score)\n",
    "        real_label = np.append(real_label, labels.numpy()[0])\n",
    "        \n",
    "        rec_loss_list = np.append(rec_loss_list, loss_rec)\n",
    "        feat_loss_list = np.append(feat_loss_list, loss_feat)\n",
    "        if (counter % 100) == 0:\n",
    "            print(counter, \" tested.\")\n",
    "    ''' Scale scores vector between [0, 1]'''\n",
    "    scores_ano = (scores_ano - scores_ano.min())/(scores_ano.max()-scores_ano.min())\n",
    "    \n",
    "    auc_out, threshold = roc(real_label, scores_ano, name_model)\n",
    "    print(\"auc: \", auc_out)\n",
    "    print(\"threshold: \", threshold)\n",
    "\n",
    "\n",
    "\n",
    "    scores_ano = (scores_ano > threshold).astype(int)\n",
    "    cm = tf.math.confusion_matrix(labels=real_label, predictions=scores_ano).numpy()\n",
    "    TP = cm[1][1]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[0][0]\n",
    "    print(cm)\n",
    "    print(\n",
    "            \"model saved. TP %d:, FP=%d, FN=%d, TN=%d\" % (TP, FP, FN, TN)\n",
    "    )\n",
    "    plot_confusion_matrix(cm, class_names, title=name_model)\n",
    "\n",
    "    diagonal_sum = cm.trace()\n",
    "    sum_of_all_elements = cm.sum()\n",
    "\n",
    "    print(\"Accuracy: \", diagonal_sum / sum_of_all_elements )\n",
    "    print(\"False Alarm Rate (FPR): \", FP/(FP+TN))\n",
    "    print(\"Leakage Rat (FNR): \", FN/(FN+TP))\n",
    "    print(\"TNR: \", TN/(FP+TN))\n",
    "    print(\"precision_score: \", TP/(TP+FP))\n",
    "    print(\"recall_score: \", TP/(TP+FN))\n",
    "    print(\"NPV: \", TN/(FN+TN))\n",
    "    print(\"F1-Score: \", f1_score(real_label, scores_ano))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4daa4c-f180-4134-ab4d-372cadd211b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMG_H, IMG_W, IMG_C)\n",
    "# set input \n",
    "inputs = tf.keras.layers.Input(input_shape, name=\"input_1\")\n",
    "# inputs_disc = tf.keras.layers.Input((IMG_H, IMG_W, 1), name=\"input_1\")\n",
    "\n",
    "g_model = build_seresnet50_unet(inputs)\n",
    "d_model = build_discriminator(inputs)\n",
    "# grayscale_converter = tf.keras.layers.Lambda(lambda x: tf.image.rgb_to_grayscale(x))\n",
    "d_model.compile()\n",
    "g_model.compile()\n",
    "g_optimizer = GCAdam(learning_rate=learning_rate, beta_1=0.5, beta_2=0.999)\n",
    "# g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "d_optimizer = GCAdam(learning_rate=learning_rate, beta_1=0.5, beta_2=0.999)\n",
    "# d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920087d-5202-4ef1-8ad8-13e33e71a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADV_REG_RATE_LF = 1\n",
    "REC_REG_RATE_LF = 50\n",
    "SSIM_REG_RATE_LF = 10\n",
    "FEAT_REG_RATE_LF = 1\n",
    "\n",
    "\n",
    "gen_loss_list = []\n",
    "disc_loss_list = []\n",
    "iter_list = []\n",
    "auc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b491a4-8a36-4df0-ac01-990231e49246",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # tf.print(\"Images: \", images)\n",
    "        reconstructed_images = g_model(real_images, training=True)\n",
    "        \n",
    "        # real_images = grayscale_converter(real_images)\n",
    "        feature_real, label_real = d_model(real_images, training=True)\n",
    "        # print(generated_images.shape)\n",
    "        feature_fake, label_fake = d_model(reconstructed_images, training=True)\n",
    "\n",
    "        discriminator_fake_average_out = tf.math.reduce_mean(label_fake, axis=0)\n",
    "        discriminator_real_average_out = tf.math.reduce_mean(label_real, axis=0)\n",
    "        real_fake_ra_out = label_real - discriminator_fake_average_out\n",
    "        fake_real_ra_out = label_fake - discriminator_real_average_out\n",
    "        epsilon = 0.000001\n",
    "        \n",
    "        # Loss 1: \n",
    "        # use relativistic average loss\n",
    "        loss_gen_ra = -( \n",
    "            tf.math.reduce_mean( \n",
    "                tf.math.log( \n",
    "                    tf.math.sigmoid(real_fake_ra_out) + epsilon), axis=0 \n",
    "            ) + tf.math.reduce_mean( \n",
    "                tf.math.log(1-tf.math.sigmoid(real_fake_ra_out) + epsilon), axis=0 \n",
    "            ) \n",
    "        )\n",
    "\n",
    "        loss_disc_ra = -( \n",
    "            tf.math.reduce_mean( \n",
    "                tf.math.log(\n",
    "                    tf.math.sigmoid(real_fake_ra_out) + epsilon), axis=0 \n",
    "            ) + tf.math.reduce_mean( \n",
    "                tf.math.log(1-tf.math.sigmoid(real_fake_ra_out) + epsilon), axis=0 \n",
    "            ) \n",
    "        )\n",
    "\n",
    "        # Loss 2: RECONSTRUCTION loss (L1)\n",
    "        loss_rec = mae(real_images, reconstructed_images)\n",
    "\n",
    "        # Loss 3: SSIM Loss\n",
    "        loss_ssim =  ssim(real_images, reconstructed_images)\n",
    "\n",
    "        # Loss 4: FEATURE Loss\n",
    "        # loss_feat = mse(feature_real, feature_fake)\n",
    "        loss_feat = multimse(feature_real, feature_fake, FEAT_REG_RATE_LF)\n",
    "\n",
    "\n",
    "        gen_loss = tf.reduce_mean( \n",
    "            (loss_gen_ra * ADV_REG_RATE_LF) \n",
    "            + (loss_rec * REC_REG_RATE_LF) \n",
    "            + (loss_ssim * SSIM_REG_RATE_LF) \n",
    "            + (loss_feat) \n",
    "        )\n",
    "\n",
    "        disc_loss = tf.reduce_mean( (loss_disc_ra * ADV_REG_RATE_LF) + (loss_feat * FEAT_REG_RATE_LF) )\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, d_model.trainable_variables)\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, g_model.trainable_variables)\n",
    "\n",
    "    d_optimizer.apply_gradients(zip(gradients_of_discriminator, d_model.trainable_variables))\n",
    "    g_optimizer.apply_gradients(zip(gradients_of_generator, g_model.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826985e9-9daa-4833-99d2-7c5fd97345b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    print(\"Start Trainning. \", name_model)\n",
    "    best_auc = 0.7\n",
    "    for meta_iter in range(meta_iters):\n",
    "        frac_done = meta_iter / meta_iters\n",
    "        cur_meta_step_size = (1 - frac_done) * meta_step_size\n",
    "        # Temporarily save the weights from the model.\n",
    "        d_old_vars = d_model.get_weights()\n",
    "        g_old_vars = g_model.get_weights()\n",
    "        # Get a sample from the full dataset.\n",
    "        mini_dataset = train_dataset.get_mini_dataset(\n",
    "            inner_batch_size, inner_iters, train_shots, classes\n",
    "        )\n",
    "        gen_loss_out = 0.0\n",
    "        disc_loss_out = 0.0\n",
    "        \n",
    "        # print(\"meta_iter: \", meta_iter)\n",
    "        for images, _ in mini_dataset:\n",
    "            g_loss, d_loss = train_step(images)\n",
    "            gen_loss_out = g_loss\n",
    "            disc_loss_out = d_loss\n",
    "            \n",
    "        d_new_vars = d_model.get_weights()\n",
    "        g_new_vars = g_model.get_weights()\n",
    "\n",
    "        # Perform SGD for the meta step.\n",
    "        for var in range(len(d_new_vars)):\n",
    "            d_new_vars[var] = d_old_vars[var] + (\n",
    "                (d_new_vars[var] - d_old_vars[var]) * cur_meta_step_size\n",
    "            )\n",
    "\n",
    "        for var in range(len(g_new_vars)):\n",
    "            g_new_vars[var] = g_old_vars[var] + (\n",
    "                (g_new_vars[var] - g_old_vars[var]) * cur_meta_step_size\n",
    "            )\n",
    "\n",
    "        # After the meta-learning step, reload the newly-trained weights into the model.\n",
    "        g_model.set_weights(g_new_vars)\n",
    "        d_model.set_weights(d_new_vars)\n",
    "        \n",
    "        # Evaluation loop\n",
    "        meta_iter = meta_iter + 1\n",
    "        if meta_iter % 100 == 0:\n",
    "            eval_g_model = g_model\n",
    "            eval_d_model = d_model\n",
    "            \n",
    "            iter_list = np.append(iter_list, meta_iter)\n",
    "            gen_loss_list = np.append(gen_loss_list, gen_loss_out)\n",
    "            disc_loss_list = np.append(disc_loss_list, disc_loss_out)\n",
    "\n",
    "            scores_ano = []\n",
    "            real_label = []\n",
    "            counter = 0\n",
    "           \n",
    "            for images, labels in eval_ds:\n",
    "\n",
    "                loss_rec, loss_feat = 0.0, 0.0\n",
    "                score = 0\n",
    "                counter += 1\n",
    "\n",
    "                '''for normal'''\n",
    "                # temp_score, loss_rec, loss_feat = calculate_a_score(eval_g_model, eval_d_model, images)\n",
    "                # score = temp_score.numpy()\n",
    "\n",
    "\n",
    "                '''for Sliding Images & LR Crop'''\n",
    "                for image in images:\n",
    "                    r_score, r_rec_loss, r_feat_loss = calculate_a_score(eval_g_model, eval_d_model, image)\n",
    "                    if r_score.numpy() > score or score == 0:\n",
    "                        score = r_score.numpy()\n",
    "                        loss_rec = r_rec_loss\n",
    "                        loss_feat = r_feat_loss\n",
    "                    \n",
    "                scores_ano = np.append(scores_ano, score)\n",
    "                real_label = np.append(real_label, labels.numpy()[0])\n",
    "                if (counter % 100) == 0:\n",
    "                    print(counter, \" tested.\")\n",
    "            # print(\"scores_ano:\", scores_ano)\n",
    "            '''Scale scores vector between [0, 1]'''\n",
    "            scores_ano = (scores_ano - scores_ano.min())/(scores_ano.max()-scores_ano.min())\n",
    "            # print(\"real_label:\", real_label)\n",
    "            # print(\"scores_ano:\", scores_ano)\n",
    "            auc_out, threshold = roc(real_label, scores_ano, name_model)\n",
    "            auc_list = np.append(auc_list, auc_out)\n",
    "            scores_ano = (scores_ano > threshold).astype(int)\n",
    "            cm = tf.math.confusion_matrix(labels=real_label, predictions=scores_ano).numpy()\n",
    "            TP = cm[1][1]\n",
    "            FP = cm[0][1]\n",
    "            FN = cm[1][0]\n",
    "            TN = cm[0][0]\n",
    "            # print(cm)\n",
    "            print(\n",
    "                f\"model saved. batch {meta_iter}:, AUC={auc_out:.3f}, TP={TP}, TN={TN}, FP={FP}, FN={FN}, Gen Loss={gen_loss_out:.5f}, Disc Loss={disc_loss_out:.5f}\" \n",
    "            )\n",
    "            \n",
    "            if auc_out >= best_auc:\n",
    "                print(\n",
    "                    f\"the best model saved. at batch {meta_iter}: with AUC={auc_out:.3f}\"\n",
    "                )\n",
    "                \n",
    "                best_g_model_path = g_model_path.replace(\".h5\", f\"_best_{meta_iter}_{auc_out:.2f}.h5\")\n",
    "                best_d_model_path = d_model_path.replace(\".h5\", f\"_best_{meta_iter}_{auc_out:.2f}.h5\")\n",
    "                g_model.save(best_g_model_path)\n",
    "                d_model.save(best_d_model_path)\n",
    "                best_auc = auc_out\n",
    "            # save model's weights\n",
    "            g_model.save(g_model_path)\n",
    "            d_model.save(d_model_path)\n",
    "    \n",
    "    \"\"\"\n",
    "    Train Ends\n",
    "    \"\"\"\n",
    "    plot_epoch_result(iter_list, gen_loss_list, \"Generator_Loss\", name_model, \"g\")\n",
    "    plot_epoch_result(iter_list, disc_loss_list, \"Discriminator_Loss\", name_model, \"r\")\n",
    "    plot_epoch_result(iter_list, auc_list, \"AUC\", name_model, \"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1b622-6b6f-430a-9bac-1c4e26e2a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking_gen_disc(name_model, g_model, d_model, g_model_path, d_model_path, test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85eee9f-0063-4c06-9e67-f2eed29faf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(test_data_path, training=False, limit=LIMIT_TEST_IMAGES)\n",
    "\n",
    "testing(g_model, d_model, g_model_path, d_model_path, test_dataset.get_dataset(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
