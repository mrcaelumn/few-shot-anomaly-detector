{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fbb1f7-1ac7-4293-8761-b388f2ef5fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc299db8-c95f-470e-89ae-2f41a6cb6301",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_H = 64\n",
    "IMG_W = 64\n",
    "IMG_C = 3  ## Change this to 1 for grayscale.\n",
    "\n",
    "# Weight initializers for the Generator network\n",
    "WEIGHT_INIT = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.2)\n",
    "\n",
    "latent_dim = 128\n",
    "    \n",
    "learning_rate = 0.00001\n",
    "meta_step_size = 0.25\n",
    "\n",
    "inner_batch_size = 25\n",
    "eval_batch_size = 25\n",
    "\n",
    "meta_iters = 100000\n",
    "eval_iters = 5\n",
    "inner_iters = 4\n",
    "\n",
    "eval_interval = 1\n",
    "train_shots = 20\n",
    "shots = 10\n",
    "classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229876d-50de-4906-ba58-519932cfec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCAdam(tf.keras.optimizers.Adam):\n",
    "    def get_gradients(self, loss, params):\n",
    "        # We here just provide a modified get_gradients() function since we are\n",
    "        # trying to just compute the centralized gradients.\n",
    "\n",
    "        grads = []\n",
    "        gradients = super().get_gradients()\n",
    "        for grad in gradients:\n",
    "            grad_len = len(grad.shape)\n",
    "            if grad_len > 1:\n",
    "                axis = list(range(grad_len - 1))\n",
    "                grad -= tf.reduce_mean(grad, axis=axis, keep_dims=True)\n",
    "            grads.append(grad)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47377645-0e75-4ee5-881c-99f25b96b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(examples, epoch, n):\n",
    "    examples = (examples + 1) / 2.0\n",
    "    for i in range(n * n):\n",
    "        plt.subplot(n, n, i+1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(examples[i])  ## pyplot.imshow(np.squeeze(examples[i], axis=-1))\n",
    "    filename = f\"samples/generated_plot_epoch-{epoch}.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241a868-2a33-42de-b550-605c148710c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    # This class will facilitate the creation of a few-shot dataset\n",
    "    # from the Omniglot dataset that can be sampled from quickly while also\n",
    "    # allowing to create new labels at the same time.\n",
    "    def __init__(self, training):\n",
    "        # Download the tfrecord files containing the omniglot data and convert to a\n",
    "        # dataset.\n",
    "        split = \"train\" if training else \"test\"\n",
    "        ds = tfds.load(\"tf_flowers\", split=split, as_supervised=True, shuffle_files=False)\n",
    "        # Iterate over the dataset to get each individual image and its class,\n",
    "        # and put that data into a dictionary.\n",
    "        self.data = {}\n",
    "\n",
    "        def extraction(image, label):\n",
    "            # This function will shrink the Omniglot images to the desired size,\n",
    "            # scale pixel values and convert the RGB image to grayscale\n",
    "            image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "            image = tf.image.rgb_to_grayscale(image)\n",
    "            image = tf.image.resize(image, [IMG_H, IMG_W])\n",
    "            return image, label\n",
    "\n",
    "        for image, label in ds.map(extraction):\n",
    "            image = image.numpy()\n",
    "            label = str(label.numpy())\n",
    "            if label not in self.data:\n",
    "                self.data[label] = []\n",
    "            self.data[label].append(image)\n",
    "        self.labels = list(self.data.keys())\n",
    "\n",
    "    def get_mini_dataset(\n",
    "        self, batch_size, repetitions, shots, num_classes, split=False\n",
    "    ):\n",
    "        temp_labels = np.zeros(shape=(num_classes * shots))\n",
    "        temp_images = np.zeros(shape=(num_classes * shots, IMG_H, IMG_W, IMG_C))\n",
    "        if split:\n",
    "            test_labels = np.zeros(shape=(num_classes))\n",
    "            test_images = np.zeros(shape=(num_classes, IMG_H, IMG_W, IMG_C))\n",
    "\n",
    "        # Get a random subset of labels from the entire label set.\n",
    "        label_subset = random.choices(self.labels, k=num_classes)\n",
    "        for class_idx, class_obj in enumerate(label_subset):\n",
    "            # Use enumerated index value as a temporary label for mini-batch in\n",
    "            # few shot learning.\n",
    "            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\n",
    "            # If creating a split dataset for testing, select an extra sample from each\n",
    "            # label to create the test dataset.\n",
    "            if split:\n",
    "                test_labels[class_idx] = class_idx\n",
    "                images_to_split = random.choices(\n",
    "                    self.data[label_subset[class_idx]], k=shots + 1\n",
    "                )\n",
    "                test_images[class_idx] = images_to_split[-1]\n",
    "                temp_images[\n",
    "                    class_idx * shots : (class_idx + 1) * shots\n",
    "                ] = images_to_split[:-1]\n",
    "            else:\n",
    "                # For each index in the randomly selected label_subset, sample the\n",
    "                # necessary number of images.\n",
    "                temp_images[\n",
    "                    class_idx * shots : (class_idx + 1) * shots\n",
    "                ] = random.choices(self.data[label_subset[class_idx]], k=shots)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (temp_images.astype(np.float32), temp_labels.astype(np.int32))\n",
    "        )\n",
    "        dataset = dataset.shuffle(100).batch(batch_size).repeat(repetitions)\n",
    "        if split:\n",
    "            return dataset, test_images, test_labels\n",
    "        return dataset\n",
    "\n",
    "\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings()  # Disable SSL warnings that may happen during download.\n",
    "train_dataset = Dataset(training=True)\n",
    "test_dataset = Dataset(training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeff0229-24d5-497f-9aad-ecc78421b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axarr = plt.subplots(nrows=5, ncols=5, figsize=(20, 20))\n",
    "\n",
    "sample_keys = list(train_dataset.data.keys())\n",
    "\n",
    "for a in range(5):\n",
    "    for b in range(5):\n",
    "        temp_image = train_dataset.data[sample_keys[a]][b]\n",
    "        temp_image = np.stack((temp_image[:, :, 0],) * 3, axis=2)\n",
    "        temp_image *= 255\n",
    "        temp_image = np.clip(temp_image, 0, 255).astype(\"uint8\")\n",
    "        if b == 2:\n",
    "            axarr[a, b].set_title(\"Class : \" + sample_keys[a])\n",
    "        axarr[a, b].imshow(temp_image, cmap=\"gray\")\n",
    "        axarr[a, b].xaxis.set_visible(False)\n",
    "        axarr[a, b].yaxis.set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9da365-5389-47a1-aadc-18bc0ba50f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create generator model based on resnet50 and unet network\n",
    "def build_generator(input_shape):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Random noise to 16x16x256 image\n",
    "    # model.add(tf.keras.layers.Dense(1024, activation=\"relu\", use_bias=False, input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.Dense(4*4*512, input_shape=input_shape))\n",
    "    \n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Reshape([4,4,512]))\n",
    "    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(256, (5,5),strides=(2,2),use_bias=False,padding=\"same\", kernel_initializer=WEIGHT_INIT))\n",
    "    # model.add(tf.keras.layers.Conv2D(128, (1,1),strides=(2,2), use_bias=False, padding=\"same\", kernel_initializer=WEIGHT_INIT))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "  \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(128, (5,5),strides=(2,2),use_bias=False,padding=\"same\", kernel_initializer=WEIGHT_INIT))\n",
    "    # model.add(tf.keras.layers.Conv2D(64, (1,1),strides=(2,2), use_bias=False, padding=\"same\", kernel_initializer=WEIGHT_INIT))\n",
    "    # model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(64, (5,5), strides=(2,2),use_bias=False,padding=\"same\", kernel_initializer=WEIGHT_INIT))\n",
    "    # model.add(tf.keras.layers.Conv2D(32, (1,1),strides=(2,2), use_bias=False, padding=\"same\", kernel_initializer=WEIGHT_INIT))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(3, (5,5), strides=(2,2),use_bias=False,padding=\"same\",kernel_initializer=WEIGHT_INIT,\n",
    "                                     activation=\"tanh\"\n",
    "                                    ))\n",
    "              # Tanh activation function compress values between -1 and 1. \n",
    "              # This is why we compressed our images between -1 and 1 in readImage function.\n",
    "    # assert model.output_shape == (None,128,128,3)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9aad2-6f9e-40b4-94ab-3c965a4a6f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create discriminator model\n",
    "def build_discriminator(input_shape):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(64,(5,5),strides=(2,2),padding=\"same\", input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(128,(5,5),strides=(2,2),padding=\"same\"))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(256,(5,5),strides=(2,2),padding=\"same\"))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(512,(5,5),strides=(2,2),padding=\"same\"))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553aa93-ab9e-4535-b59f-63093d6475b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.1)\n",
    "\n",
    "def wasserstein_loss(real_images, fake_images):\n",
    "\treturn tf.keras.backend.mean(real_images * fake_images)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # First argument of loss is real labels\n",
    "    # We've labeled our images as 1 (real) because\n",
    "    # we're trying to fool discriminator\n",
    "    return cross_entropy(tf.ones_like(fake_output),fake_output)\n",
    "\n",
    "\n",
    "def discriminator_loss(real_images,fake_images):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_images),real_images)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_images),fake_images)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4daa4c-f180-4134-ab4d-372cadd211b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMG_H, IMG_W, IMG_C)\n",
    "\n",
    "d_model = build_discriminator(input_shape)\n",
    "g_model = build_generator((latent_dim, ))\n",
    "d_model.compile()\n",
    "g_model.compile()\n",
    "\n",
    "g_optimizer = GCAdam(learning_rate=learning_rate, beta_1=0.5, beta_2=0.999)\n",
    "d_optimizer = GCAdam(learning_rate=learning_rate, beta_1=0.5, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826985e9-9daa-4833-99d2-7c5fd97345b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta_iter in range(meta_iters):\n",
    "    frac_done = meta_iter / meta_iters\n",
    "    cur_meta_step_size = (1 - frac_done) * meta_step_size\n",
    "    # Temporarily save the weights from the model.\n",
    "    d_old_vars = d_model.get_weights()\n",
    "    g_old_vars = g_model.get_weights()\n",
    "    # Get a sample from the full dataset.\n",
    "    mini_dataset = train_dataset.get_mini_dataset(\n",
    "        inner_batch_size, inner_iters, train_shots, classes\n",
    "    )\n",
    "    for images, labels in mini_dataset:\n",
    "        \n",
    "        noise = tf.random.normal([inner_batch_size, latent_dim])\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            # Generator generated images\n",
    "            generated_images = g_model(noise, training=True)\n",
    "\n",
    "            # We've sent our real and fake images to the discriminator\n",
    "            # and taken the decisions of it.\n",
    "            real_output = d_model(images,training=True)\n",
    "            fake_output = d_model(generated_images,training=True)\n",
    "\n",
    "            # We've computed losses of generator and discriminator\n",
    "            gen_loss = generator_loss(fake_output)\n",
    "            disc_loss = discriminator_loss(real_output,fake_output)\n",
    "\n",
    "        # We've computed gradients of networks and updated variables using those gradients.\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, g_model.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, d_model.trainable_variables)\n",
    "        d_optimizer.apply_gradients(zip(gradients_of_discriminator, d_model.trainable_variables))\n",
    "        g_optimizer.apply_gradients(zip(gradients_of_generator, g_model.trainable_variables))\n",
    "        \n",
    "        \n",
    "        \n",
    "    d_new_vars = d_model.get_weights()\n",
    "    g_new_vars = g_model.get_weights()\n",
    "    \n",
    "    # Perform SGD for the meta step.\n",
    "    for var in range(len(d_new_vars)):\n",
    "        d_new_vars[var] = d_old_vars[var] + (\n",
    "            (d_new_vars[var] - d_old_vars[var]) * cur_meta_step_size\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n",
    "    for var in range(len(g_new_vars)):\n",
    "        g_new_vars[var] = g_old_vars[var] + (\n",
    "            (g_new_vars[var] - g_old_vars[var]) * cur_meta_step_size\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # After the meta-learning step, reload the newly-trained weights into the model.\n",
    "    g_model.set_weights(g_new_vars)\n",
    "    d_model.set_weights(d_new_vars)\n",
    "    # Evaluation loop\n",
    "    if meta_iter % eval_interval == 0:\n",
    "        mini_dataset = test_dataset.get_mini_dataset(\n",
    "            inner_batch_size, inner_iters, train_shots, classes\n",
    "        )\n",
    "        \n",
    "        d_old_vars = d_model.get_weights()\n",
    "        g_old_vars = g_model.get_weights()\n",
    "        \n",
    "        for images, labels in mini_dataset:\n",
    "            noise = tf.random.normal([inner_batch_size, latent_dim])\n",
    "            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "                # Generator generated images\n",
    "                generated_images = g_model(noise, training=True)\n",
    "\n",
    "                # We've sent our real and fake images to the discriminator\n",
    "                # and taken the decisions of it.\n",
    "                real_output = d_model(images, training=True)\n",
    "                fake_output = d_model(generated_images, training=True)\n",
    "\n",
    "                # We've computed losses of generator and discriminator\n",
    "                gen_loss = generator_loss(fake_output)\n",
    "                disc_loss = discriminator_loss(real_output,fake_output)\n",
    "\n",
    "            # We've computed gradients of networks and updated variables using those gradients.\n",
    "            gradients_of_generator = gen_tape.gradient(gen_loss, g_model.trainable_variables)\n",
    "            gradients_of_discriminator = disc_tape.gradient(disc_loss, d_model.trainable_variables)\n",
    "\n",
    "            g_optimizer.apply_gradients(zip(gradients_of_generator, g_model.trainable_variables))\n",
    "            d_optimizer.apply_gradients(zip(gradients_of_discriminator, d_model.trainable_variables))\n",
    "        \n",
    "        # g_model.set_weights(g_old_vars)\n",
    "        # d_model.set_weights(d_old_vars)\n",
    "        \n",
    "        if meta_iter % 100 == 0:\n",
    "            print(\n",
    "                \"generate image in batch %d:\" % (meta_iter)\n",
    "            )\n",
    "            noise = np.random.normal(size=(inner_batch_size, latent_dim))\n",
    "            examples = g_model.predict(noise)\n",
    "            save_plot(examples, meta_iter, int(np.sqrt(inner_batch_size)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25523e1b-e6e1-4342-9076-577efe5d0b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([inner_batch_size, latent_dim])\n",
    "examples = g_model.predict(noise)\n",
    "save_plot(examples, \"few-shot-gan\", int(np.sqrt(inner_batch_size)))\n",
    "    # Train on the samples and get the resulting accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c41fe-2c5e-4dfc-8d82-d314f45781f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
