{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fbb1f7-1ac7-4293-8761-b388f2ef5fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_addons as tfa\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "import multiprocess as mp\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, precision_score, recall_score, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import math\n",
    "import natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397dfa5-eb65-419a-8044-4ddbac706329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Lambda, Activation, MaxPooling2D, MaxPool2D, \\\n",
    "GlobalAveragePooling2D, Conv2DTranspose, Concatenate, \\\n",
    "Input, Dense, Reshape, Multiply, Add, Flatten, ZeroPadding2D\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.utils.layer_utils import get_source_inputs\n",
    "from keras import backend\n",
    "\n",
    "import keras_applications as ka\n",
    "\n",
    "import collections\n",
    "\n",
    "IMG_SIZE = 512\n",
    "\n",
    "ModelParams = collections.namedtuple(\n",
    "    'ModelParams',\n",
    "    ['model_name', 'repetitions', 'residual_block', 'groups',\n",
    "     'reduction', 'init_filters', 'input_3x3', 'dropout']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc299db8-c95f-470e-89ae-2f41a6cb6301",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "ORI_SIZE = (271, 481)\n",
    "IMG_H = 128\n",
    "IMG_W = 128\n",
    "IMG_C = 3  ## Change this to 1 for grayscale.\n",
    "winSize = (256, 256)\n",
    "stSize = 20\n",
    "\n",
    "# Weight initializers for the Generator network\n",
    "# WEIGHT_INIT = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.2)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "LIMIT_EVAL_IMAGES = 100\n",
    "LIMIT_TEST_IMAGES = \"MAX\"\n",
    "LIMIT_TRAIN_IMAGES = 100\n",
    "\n",
    "TRAINING_DURATION = None\n",
    "TESTING_DURATION = None\n",
    "\n",
    "NUMBER_IMAGES_SELECTED = 1000\n",
    "\n",
    "# range between 0-1\n",
    "anomaly_weight = 0.7\n",
    "\n",
    "learning_rate = 0.002\n",
    "meta_step_size = 0.25\n",
    "\n",
    "inner_batch_size = 25\n",
    "eval_batch_size = 25\n",
    "\n",
    "meta_iters = 2000\n",
    "inner_iters = 4\n",
    "\n",
    "\n",
    "train_shots = 100\n",
    "shots = 20\n",
    "classes = 1\n",
    "n_shots = shots\n",
    "if shots > 20 :\n",
    "    n_shots = \"few\"\n",
    "    \n",
    "DATASET_NAME = \"mura\"\n",
    "NO_DATASET = 0 # 0=0-999 images, 1=1000-1999, 2=2000-2999 so on\n",
    "PERCENTAGE_COMPOSITION_DATASET = {\n",
    "    \"top\": 50,\n",
    "    \"mid\": 40,\n",
    "    \"bottom\": 10\n",
    "}\n",
    "\n",
    "mode_colour = str(IMG_H) + \"_rgb\"\n",
    "if IMG_C == 1:\n",
    "    mode_colour = str(IMG_H) + \"_gray\"\n",
    "\n",
    "model_type = \"seresnext50\"\n",
    "name_model = f\"{mode_colour}_{DATASET_NAME}_{NO_DATASET}_{model_type}_{n_shots}_shots_mura_detection_{str(meta_iters)}\"\n",
    "g_model_path = f\"saved_model/{name_model}_g_model.h5\"\n",
    "d_model_path = f\"saved_model/{name_model}_d_model.h5\"\n",
    "\n",
    "TRAIN = True\n",
    "if not TRAIN:\n",
    "    g_model_path = f\"saved_model/g_model_name.h5\"\n",
    "    d_model_path = f\"saved_model/d_model_name.h5\"\n",
    "    \n",
    "train_data_path = f\"data/{DATASET_NAME}/train_data\"\n",
    "eval_data_path = f\"data/{DATASET_NAME}/eval_data\"\n",
    "test_data_path = f\"data/{DATASET_NAME}/test_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828014f0-4825-4f9c-8e4e-c50bff9d4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for SSIM loss function\n",
    "class SSIMLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self,\n",
    "         reduction=tf.keras.losses.Reduction.AUTO,\n",
    "         name='SSIMLoss'):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "\n",
    "    def call(self, ori, recon):\n",
    "        recon = tf.convert_to_tensor(recon)\n",
    "        ori = tf.cast(ori, recon.dtype)\n",
    "\n",
    "        # Loss 3: SSIM Loss\n",
    "#         loss_ssim =  tf.reduce_mean(1 - tf.image.ssim(ori, recon, max_val=1.0)[0]) \n",
    "        loss_ssim = tf.reduce_mean(1 - tf.image.ssim(ori, recon, max_val=IMG_W, filter_size=7, k1=0.01 ** 2, k2=0.03 ** 2))\n",
    "        return loss_ssim\n",
    "    \n",
    "\n",
    "class MultiFeatureLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self,\n",
    "             reduction=tf.keras.losses.Reduction.AUTO,\n",
    "             name='FeatureLoss'):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.mse_func = tf.keras.losses.MeanSquaredError() \n",
    "\n",
    "    def call(self, real, fake, weight=1):\n",
    "        result = 0.0\n",
    "        for r, f in zip(real, fake):\n",
    "            result = result + (weight * self.mse_func(r, f))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "# class for Adversarial loss function\n",
    "class AdversarialLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self,\n",
    "             reduction=tf.keras.losses.Reduction.AUTO,\n",
    "             name='AdversarialLoss'):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "\n",
    "    def call(self, logits_in, labels_in):\n",
    "        labels_in = tf.convert_to_tensor(labels_in)\n",
    "        logits_in = tf.cast(logits_in, labels_in.dtype)\n",
    "        # Loss 4: FEATURE Loss\n",
    "        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_in, labels=labels_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f0129-2529-4ace-bbef-848d8c4f8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, name_model):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(name_model+'_roc_curve.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "''' calculate the auc value for lables and scores'''\n",
    "def roc(labels, scores, name_model):\n",
    "    \"\"\"Compute ROC curve and ROC area for each class\"\"\"\n",
    "    roc_auc = dict()\n",
    "    # True/False Positive Rates.\n",
    "    fpr, tpr, threshold = roc_curve(labels, scores)\n",
    "    # print(\"threshold: \", threshold)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # get a threshod that perform very well.\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = threshold[optimal_idx]\n",
    "    # draw plot for ROC-Curve\n",
    "    plot_roc_curve(fpr, tpr, name_model)\n",
    "    \n",
    "    return roc_auc, optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c231433-0c03-4a84-a2b5-d40b36825ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delcare all loss function that we will use\n",
    "# L1 Loss\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "# L2 Loss\n",
    "mse = tf.keras.losses.MeanSquaredError() \n",
    "\n",
    "multimse = MultiFeatureLoss()\n",
    "# SSIM loss\n",
    "ssim = SSIMLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229876d-50de-4906-ba58-519932cfec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCAdam(tf.keras.optimizers.Adam):\n",
    "    def get_gradients(self, loss, params):\n",
    "        # We here just provide a modified get_gradients() function since we are\n",
    "        # trying to just compute the centralized gradients.\n",
    "\n",
    "        grads = []\n",
    "        gradients = super().get_gradients()\n",
    "        for grad in gradients:\n",
    "            grad_len = len(grad.shape)\n",
    "            if grad_len > 1:\n",
    "                axis = list(range(grad_len - 1))\n",
    "                grad -= tf.reduce_mean(grad, axis=axis, keep_dims=True)\n",
    "            grads.append(grad)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47377645-0e75-4ee5-881c-99f25b96b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(examples, epoch, n):\n",
    "    examples = (examples + 1) / 2.0\n",
    "    for i in range(n * n):\n",
    "        plt.subplot(n, n, i+1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(examples[i])  ## pyplot.imshow(np.squeeze(examples[i], axis=-1))\n",
    "    filename = f\"samples/generated_plot_epoch-{epoch}.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f84aa3-8208-4a3c-b433-d5a2d1791301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(title+'_cm.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "def plot_epoch_result(iters, loss, name, model_name, colour):\n",
    "    plt.plot(iters, loss, colour, label=name)\n",
    "#     plt.plot(epochs, disc_loss, 'b', label='Discriminator loss')\n",
    "    plt.title(name)\n",
    "    plt.xlabel('Iters')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(model_name+ '_'+name+'_iters_result.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def plot_anomaly_score(score_ano, labels, name, model_name):\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "    {'predicts': score_ano,\n",
    "     'label': labels\n",
    "    })\n",
    "    \n",
    "    df_normal = df[df.label == 0]\n",
    "    sns.distplot(df_normal['predicts'],  kde=False, label='normal')\n",
    "\n",
    "    df_defect = df[df.label == 1]\n",
    "    sns.distplot(df_defect['predicts'],  kde=False, label='defect')\n",
    "    \n",
    "#     plt.plot(epochs, disc_loss, 'b', label='Discriminator loss')\n",
    "    plt.title(name)\n",
    "    plt.xlabel('Anomaly Scores')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.legend(prop={'size': 12})\n",
    "    plt.savefig(model_name+ '_'+name+'_anomay_scores_dist.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def write_result(array_lines, name):\n",
    "    with open(f'{name}.txt', 'w+') as f:\n",
    "        f.write('\\n'.join(array_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c55c0-2a9d-411d-8f29-cb06573417c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_by_percentage(percentage, whole):\n",
    "    return math.ceil(float(percentage)/100 * float(whole))\n",
    "\n",
    "\"\"\"\n",
    "input: array [[path_of_file <string>, label <int>]]\n",
    "output: array of path [path_of_file <string>] & array of label [label <int>]\n",
    "\"\"\"\n",
    "def selecting_images_preprocessing(images_path_array, limit_image_to_train = \"MAX\", composition={}):\n",
    "    # images_path_array = glob(images_path)\n",
    "    final_image_path = []\n",
    "    final_label = []\n",
    "    def processing_image(img_data):\n",
    "        img_path = img_data[0]\n",
    "        label = img_data[1]\n",
    "        # print(img_path, label)\n",
    "        image = cv2.imread(img_path)\n",
    "        # print(image)\n",
    "        mean = np.mean(image)\n",
    "        std = np.std(image)\n",
    "        # print(mean, image.mean())\n",
    "        # print(std, image.std())\n",
    "        data_row = {\n",
    "            \"image_path\": img_path,\n",
    "            \"mean\": image.mean(),\n",
    "            \"std\": image.std(),\n",
    "            \"class\": label\n",
    "        }\n",
    "        # print(data_row)\n",
    "        return data_row\n",
    "    \n",
    "        \n",
    "    print(\"processed number of data: \", len(images_path_array))\n",
    "    if limit_image_to_train == \"MAX\":\n",
    "        limit_image_to_train = len(images_path_array)\n",
    "            \n",
    "    df_analysis = pd.DataFrame(columns=['image_path','mean','std', 'class'])\n",
    "    \n",
    "    # multiple processing calculating std\n",
    "    \n",
    "    pool = mp.Pool(5)\n",
    "    data_rows = pool.map(processing_image, images_path_array)\n",
    "    \n",
    "    df_analysis = df_analysis.append(data_rows, ignore_index = True)\n",
    "            \n",
    "    final_df = df_analysis.sort_values(['std', 'mean'], ascending = [True, False])\n",
    "    \n",
    "    if composition == {}:\n",
    "        final_df = shuffle(final_df)\n",
    "        final_image_path = final_df['image_path'].head(limit_image_to_train).tolist()\n",
    "        final_label = final_df['class'].head(limit_image_to_train).tolist()\n",
    "    else:\n",
    "        counter_available_no_data = limit_image_to_train\n",
    "        if composition.get('top') != 0:\n",
    "            num_rows = get_number_by_percentage(composition.get('top'), limit_image_to_train)\n",
    "            if counter_available_no_data <= num_rows:\n",
    "                num_rows = counter_available_no_data\n",
    "            counter_available_no_data = counter_available_no_data - num_rows\n",
    "            \n",
    "            print(composition.get('top'), num_rows, counter_available_no_data)\n",
    "            \n",
    "            # get top data\n",
    "            final_image_path = final_image_path + final_df['image_path'].head(num_rows).tolist()\n",
    "            final_label = final_label + final_df['class'].head(num_rows).tolist()\n",
    "            \n",
    "        if composition.get('mid') != 0:\n",
    "            num_rows = get_number_by_percentage(composition.get('mid'), limit_image_to_train)\n",
    "            if counter_available_no_data <= num_rows:\n",
    "                num_rows = counter_available_no_data\n",
    "            counter_available_no_data = counter_available_no_data - num_rows\n",
    "            \n",
    "            print(composition.get('mid'), num_rows, counter_available_no_data)\n",
    "            \n",
    "            # top & mid\n",
    "            n = len(final_df.index)\n",
    "            mid_n = round(n/2)\n",
    "            mid_k = round(num_rows/2)\n",
    "\n",
    "            start = mid_n - mid_k\n",
    "            end = mid_n + mid_k\n",
    "\n",
    "            final = final_df.iloc[start:end]\n",
    "            final_image_path = final_image_path + final['image_path'].head(num_rows).tolist()\n",
    "            final_label = final_label + final['class'].head(num_rows).tolist()\n",
    "            \n",
    "        if composition.get('bottom') != 0:\n",
    "            num_rows = get_number_by_percentage(composition.get('bottom'), limit_image_to_train)\n",
    "            if counter_available_no_data <= num_rows:\n",
    "                num_rows = counter_available_no_data\n",
    "            counter_available_no_data = counter_available_no_data - num_rows\n",
    "            \n",
    "            print(composition.get('bottom'), num_rows, counter_available_no_data)\n",
    "            \n",
    "            # get bottom data\n",
    "            final_image_path = final_image_path + final_df['image_path'].tail(num_rows).tolist()\n",
    "            final_label = final_label + final_df['class'].tail(num_rows).tolist()\n",
    "    \n",
    "    \n",
    "    # clear zombies memory\n",
    "    del [[final_df, df_analysis]]\n",
    "    gc.collect()\n",
    "    \n",
    "    # print(final_image_path, final_label)\n",
    "    # print(len(final_image_path), len(final_label))\n",
    "    return final_image_path, final_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa049b05-f27c-45be-949a-c362baa633dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_image(image, beta=0.1):\n",
    "    image = tf.cast(image, tf.float64)\n",
    "    image = ((1 + beta) * image) + (-beta * tf.math.reduce_mean(image))\n",
    "    # image = ((1 + beta) * image) + (-beta * np.mean(image))\n",
    "    return image\n",
    "\n",
    "def sliding_crop_and_select_one(img, stepSize=stSize, windowSize=winSize):\n",
    "    current_std = 0\n",
    "    current_image = None\n",
    "    y_end_crop, x_end_crop = False, False\n",
    "    for y in range(0, ORI_SIZE[0], stepSize):\n",
    "        \n",
    "        y_end_crop = False\n",
    "        \n",
    "        for x in range(0, ORI_SIZE[1], stepSize):\n",
    "            \n",
    "            x_end_crop = False\n",
    "            \n",
    "            crop_y = y\n",
    "            if (y + windowSize[0]) > ORI_SIZE[0]:\n",
    "                crop_y =  ORI_SIZE[0] - windowSize[0]\n",
    "                y_end_crop = True\n",
    "            \n",
    "            crop_x = x\n",
    "            if (x + windowSize[1]) > ORI_SIZE[1]:\n",
    "                crop_x = ORI_SIZE[1] - windowSize[1]\n",
    "                x_end_crop = True\n",
    "                \n",
    "            image = tf.image.crop_to_bounding_box(img, crop_y, crop_x, windowSize[0], windowSize[1])                \n",
    "            std_image = tf.math.reduce_std(tf.cast(image, dtype=tf.float32))\n",
    "          \n",
    "            if current_std == 0 or std_image < current_std :\n",
    "                current_std = std_image\n",
    "                current_image = image\n",
    "                \n",
    "            if x_end_crop:\n",
    "                break\n",
    "                \n",
    "        if x_end_crop and y_end_crop:\n",
    "            break\n",
    "            \n",
    "    return current_image\n",
    "\n",
    "def sliding_crop(img, stepSize=stSize, windowSize=winSize):\n",
    "    current_std = 0\n",
    "    current_image = []\n",
    "    y_end_crop, x_end_crop = False, False\n",
    "    for y in range(0, ORI_SIZE[0], stepSize):\n",
    "        y_end_crop = False\n",
    "        for x in range(0, ORI_SIZE[1], stepSize):\n",
    "            x_end_crop = False\n",
    "            crop_y = y\n",
    "            if (y + windowSize[0]) > ORI_SIZE[0]:\n",
    "                crop_y =  ORI_SIZE[0] - windowSize[0]\n",
    "            \n",
    "            crop_x = x\n",
    "            if (x + windowSize[1]) > ORI_SIZE[1]:\n",
    "                crop_x = ORI_SIZE[1] - windowSize[1]\n",
    "            \n",
    "            # print(crop_y, crop_x, windowSize)\n",
    "            image = tf.image.crop_to_bounding_box(img, crop_y, crop_x, windowSize[0], windowSize[1])\n",
    "            current_image.append(image)\n",
    "            if x_end_crop:\n",
    "                break\n",
    "        if x_end_crop and y_end_crop:\n",
    "            break\n",
    "    return current_image\n",
    "\n",
    "def custom_v3(img):\n",
    "    img = tf.image.adjust_gamma(img)\n",
    "    img = tfa.image.median_filter2d(img, 3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80474f87-23e7-4760-9a05-1d76ca4d740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_with_labels(filepath, class_names, training=True, limit=100):\n",
    "   \n",
    "    image_list = []\n",
    "    label_list = []\n",
    "    for class_n in class_names:  # do dogs and cats\n",
    "        path = os.path.join(filepath,class_n)  # create path to dogs and cats\n",
    "        class_num = class_names.index(class_n)  # get the classification  (0 or a 1). 0=dog 1=cat\n",
    "        path_list = []\n",
    "        class_list = []\n",
    "        \n",
    "        list_path = natsort.natsorted(os.listdir(path))\n",
    "        \n",
    "        if training:\n",
    "            print(\"total number of dataset\", len(list_path))\n",
    "\n",
    "            newarr_list_path = np.array_split(list_path, math.ceil(len(list_path)/NUMBER_IMAGES_SELECTED))\n",
    "\n",
    "            print(\"number of sub dataset\", len(newarr_list_path))\n",
    "\n",
    "            list_path = newarr_list_path[NO_DATASET]\n",
    "\n",
    "            print(\"data taken from dataset\", len(list_path))\n",
    "        \n",
    "        \n",
    "        for img in tqdm(list_path, desc='selecting images'):  \n",
    "            if \".DS_Store\" != img:\n",
    "                # print(img)\n",
    "                filpath = os.path.join(path,img)\n",
    "#                 print(filpath, class_num)\n",
    "                \n",
    "                path_list.append(filpath)\n",
    "                class_list.append(class_num)\n",
    "                # image_label_list.append({filpath:class_num})\n",
    "        \n",
    "        n_samples = None\n",
    "        if limit != \"MAX\":\n",
    "            n_samples = limit\n",
    "        else: \n",
    "            n_samples = len(path_list)\n",
    "            \n",
    "        if training:\n",
    "            ''' \n",
    "            selecting by attribute of image\n",
    "            '''\n",
    "            combined = np.transpose((path_list, class_list))\n",
    "            # print(combined)\n",
    "            path_list, class_list = selecting_images_preprocessing(combined, limit_image_to_train=n_samples, composition=PERCENTAGE_COMPOSITION_DATASET)\n",
    "        \n",
    "        else:\n",
    "            ''' \n",
    "            random selecting\n",
    "            '''\n",
    "            path_list, class_list = shuffle(path_list, class_list, n_samples=n_samples ,random_state=random.randint(123, 10000))\n",
    "        \n",
    "        image_list = image_list + path_list\n",
    "        label_list = label_list + class_list\n",
    "  \n",
    "    # print(image_list, label_list)\n",
    "    \n",
    "    return image_list, label_list\n",
    "\n",
    "def prep_stage(x, train=True):\n",
    "    beta_contrast = 0.1\n",
    "    # enchance the brightness\n",
    "    x = enhance_image(x, beta_contrast)\n",
    "    # if train:\n",
    "        # x = enhance_image(x, beta_contrast)\n",
    "        # x = tfa.image.equalize(x)\n",
    "        # x = custom_v3(x)\n",
    "    # else: \n",
    "        # x = enhance_image(x, beta_contrast)\n",
    "        # x = tfa.image.equalize(x)\n",
    "        # x = custom_v3(x)\n",
    "        \n",
    "    return x\n",
    "\n",
    "def post_stage(x):\n",
    "    \n",
    "    x = tf.image.resize(x, (IMG_H, IMG_W))\n",
    "    # x = tf.image.resize_with_crop_or_pad(x, IMG_H, IMG_W)\n",
    "    # normalize to the range -1,1\n",
    "    # x = tf.cast(x, tf.float32)\n",
    "    x = (x - 127.5) / 127.5\n",
    "    # normalize to the range 0-1\n",
    "    # img /= 255.0\n",
    "    return x\n",
    "\n",
    "def extraction(image, label):\n",
    "    # This function will shrink the Omniglot images to the desired size,\n",
    "    # scale pixel values and convert the RGB image to grayscale\n",
    "    img = tf.io.read_file(image)\n",
    "    img = tf.io.decode_png(img, channels=IMG_C)\n",
    "    # print(image, label)\n",
    "    # img = cv2.imread(image)\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img = prep_stage(img, True)\n",
    "    \n",
    "    img = sliding_crop_and_select_one(img)\n",
    "    img = post_stage(img)\n",
    "\n",
    "    return img, label\n",
    "\n",
    "def extraction_test(image, label):\n",
    "    # This function will shrink the Omniglot images to the desired size,\n",
    "    # scale pixel values and convert the RGB image to grayscale\n",
    "    img = tf.io.read_file(image)\n",
    "    img = tf.io.decode_png(img, channels=IMG_C)\n",
    "    # img = cv2.imread(image)\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img = prep_stage(img, False)\n",
    "    # img = post_stage(img)\n",
    "    \n",
    "    img_list = sliding_crop(img)\n",
    "    \n",
    "    img = [post_stage(a) for a in img_list]\n",
    "\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd40c6-f382-409e-822e-9fa683bf0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_gen_disc(mode, g_model_inner, d_model_inner, g_filepath, d_filepath, test_data_path):\n",
    "    print(\"Start Checking Reconstructed Image\")\n",
    "    g_model_inner.load_weights(g_filepath)\n",
    "    d_model_inner.load_weights(d_filepath)\n",
    "    \n",
    "    normal_image = glob(test_data_path+\"/normal/*.png\")[0]\n",
    "    defect_image = glob(test_data_path+\"/defect/*.png\")[0]\n",
    "    paths = {\n",
    "        \"normal\": normal_image,\n",
    "        \"defect\": defect_image,\n",
    "    }\n",
    "\n",
    "    for i, v in paths.items():\n",
    "        print(i,v)\n",
    "\n",
    "        width=IMG_W\n",
    "        height=IMG_H\n",
    "        rows = 1\n",
    "        cols = 3\n",
    "        axes=[]\n",
    "        fig = plt.figure()\n",
    "\n",
    "        \n",
    "        img, label = extraction(v, i)\n",
    "       \n",
    "        name_subplot = mode+'_original_'+i\n",
    "        axes.append( fig.add_subplot(rows, cols, 1) )\n",
    "        axes[-1].set_title('_original_')  \n",
    "        \n",
    "        img = np.clip(img.numpy(), 0, 1)\n",
    "        \n",
    "        plt.imshow(img.astype(np.uint8), alpha=1.0)\n",
    "        plt.axis('off')\n",
    "\n",
    "       \n",
    "        img = tf.cast(img, tf.float64)\n",
    "        img = (img - 127.5) / 127.5\n",
    "\n",
    "\n",
    "        image = tf.reshape(img, (-1, IMG_H, IMG_W, IMG_C))\n",
    "        reconstructed_images = g_model_inner.predict(image)\n",
    "        reconstructed_images = tf.reshape(reconstructed_images, (IMG_H, IMG_W, IMG_C))\n",
    "        reconstructed_images = reconstructed_images * 127 + 127\n",
    "\n",
    "        name_subplot = mode+'_reconstructed_'+i\n",
    "        axes.append( fig.add_subplot(rows, cols, 3) )\n",
    "        axes[-1].set_title('_reconstructed_') \n",
    "        \n",
    "        reconstructed_images = np.clip(reconstructed_images.numpy(), 0, 1)\n",
    "        \n",
    "        plt.imshow(reconstructed_images.astype(np.uint8), alpha=1.0)\n",
    "        plt.axis('off')\n",
    "\n",
    "        fig.tight_layout()    \n",
    "        fig.savefig(mode+'_'+i+'.png')\n",
    "        plt.show()\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241a868-2a33-42de-b550-605c148710c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    # This class will facilitate the creation of a few-shot dataset\n",
    "    # from the Omniglot dataset that can be sampled from quickly while also\n",
    "    # allowing to create new labels at the same time.\n",
    "    def __init__(self, path_file, training=True, limit=100):\n",
    "        # Download the tfrecord files containing the omniglot data and convert to a\n",
    "        # dataset.\n",
    "        start_time = datetime.now()\n",
    "        self.data = {}\n",
    "        class_names = [\"normal\"] if training else [\"normal\", \"defect\"]\n",
    "        filenames, labels = read_data_with_labels(path_file, class_names, training, limit)\n",
    "        \n",
    "        ds = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "        self.ds = ds.shuffle(buffer_size=1024, seed=random.randint(123, 10000) )\n",
    "             \n",
    "        if training:\n",
    "            for image, label in ds.map(extraction):\n",
    "                image = image.numpy()\n",
    "                label = str(label.numpy())\n",
    "                if label not in self.data:\n",
    "                    self.data[label] = []\n",
    "                self.data[label].append(image)\n",
    "            self.labels = list(self.data.keys())\n",
    "            \n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        print('classes: ', class_names)\n",
    "        print(f'(Loading Dataset and Preprocessing) Duration of counting std and mean of images: {end_time - start_time}')\n",
    "        \n",
    "\n",
    "    def get_mini_dataset(\n",
    "        self, batch_size, repetitions, shots, num_classes, split=False\n",
    "    ):\n",
    "        temp_labels = np.zeros(shape=(num_classes * shots))\n",
    "        temp_images = np.zeros(shape=(num_classes * shots, IMG_H, IMG_W, IMG_C))\n",
    "        if split:\n",
    "            test_labels = np.zeros(shape=(num_classes))\n",
    "            test_images = np.zeros(shape=(num_classes, IMG_H, IMG_W, IMG_C))\n",
    "\n",
    "        # Get a random subset of labels from the entire label set.\n",
    "        label_subset = random.choices(self.labels, k=num_classes)\n",
    "        for class_idx, class_obj in enumerate(label_subset):\n",
    "            # Use enumerated index value as a temporary label for mini-batch in\n",
    "            # few shot learning.\n",
    "            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\n",
    "            # If creating a split dataset for testing, select an extra sample from each\n",
    "            # label to create the test dataset.\n",
    "            if split:\n",
    "                test_labels[class_idx] = class_idx\n",
    "                images_to_split = random.choices(\n",
    "                    self.data[label_subset[class_idx]], k=shots + 1\n",
    "                )\n",
    "                test_images[class_idx] = images_to_split[-1]\n",
    "                temp_images[\n",
    "                    class_idx * shots : (class_idx + 1) * shots\n",
    "                ] = images_to_split[:-1]\n",
    "            else:\n",
    "                # For each index in the randomly selected label_subset, sample the\n",
    "                # necessary number of images.\n",
    "                temp_images[\n",
    "                    class_idx * shots : (class_idx + 1) * shots\n",
    "                ] = random.choices(self.data[label_subset[class_idx]], k=shots)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (temp_images.astype(np.float32), temp_labels.astype(np.int32))\n",
    "        )\n",
    "        dataset = dataset.shuffle(100, seed=int(round(datetime.now().timestamp()))).batch(batch_size).repeat(repetitions)\n",
    "        \n",
    "        if split:\n",
    "            return dataset, test_images, test_labels\n",
    "        return dataset\n",
    "    \n",
    "    def get_dataset(self, batch_size):\n",
    "        ds = self.ds.map(extraction_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        # ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "        ds = ds.batch(batch_size)\n",
    "        ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings() # Disable SSL warnings that may happen during download.\n",
    "\n",
    "## load dataset\n",
    "train_dataset = Dataset(train_data_path, training=True, limit=LIMIT_TRAIN_IMAGES)\n",
    "\n",
    "eval_dataset = Dataset(eval_data_path, training=False, limit=LIMIT_EVAL_IMAGES)\n",
    "eval_ds = eval_dataset.get_dataset(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeff0229-24d5-497f-9aad-ecc78421b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, axarr = plt.subplots(nrows=2, ncols=5, figsize=(20, 20))\n",
    "# sample_keys = list(test_dataset.data.keys())\n",
    "# # print(sample_keys)\n",
    "# for a in range(2):\n",
    "#     for b in range(5):\n",
    "#         temp_image = test_dataset.data[sample_keys[a]][b]\n",
    "#         temp_image = np.stack((temp_image[:, :, 0],) * 3, axis=2)\n",
    "#         temp_image *= 255\n",
    "#         temp_image = np.clip(temp_image, 0, 255).astype(\"uint8\")\n",
    "#         if b == 2:\n",
    "#             axarr[a, b].set_title(\"Class : \" + sample_keys[a])\n",
    "#         axarr[a, b].imshow(temp_image)\n",
    "#         axarr[a, b].xaxis.set_visible(False)\n",
    "#         axarr[a, b].yaxis.set_visible(False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19900ec8-7ac3-441e-b030-c0e7cda96824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_a_score(out_g_model, out_d_model, images):\n",
    "    reconstructed_images = out_g_model(images, training=False)\n",
    "\n",
    "    feature_real, label_real  = out_d_model(images, training=False)\n",
    "    # print(generated_images.shape)\n",
    "    feature_fake, label_fake = out_d_model(reconstructed_images, training=False)\n",
    "\n",
    "    # Loss 2: RECONSTRUCTION loss (L1)\n",
    "    loss_rec = mae(images, reconstructed_images)\n",
    "\n",
    "    loss_feat = multimse(feature_real, feature_fake)\n",
    "    # print(\"loss_rec:\", loss_rec, \"loss_feat:\", loss_feat)\n",
    "    score = (anomaly_weight * loss_rec) + ((1-anomaly_weight) * loss_feat)\n",
    "    return score, loss_rec, loss_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9da365-5389-47a1-aadc-18bc0ba50f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bn_params(**params):\n",
    "    axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "    default_bn_params = {\n",
    "        'axis': axis,\n",
    "        'epsilon': 9.999999747378752e-06,\n",
    "    }\n",
    "    default_bn_params.update(params)\n",
    "    return default_bn_params\n",
    "\n",
    "\n",
    "def get_num_channels(tensor):\n",
    "    channels_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "    return backend.int_shape(tensor)[channels_axis]\n",
    "\n",
    "def expand_dims(x, channels_axis):\n",
    "    if channels_axis == 3:\n",
    "        return x[:, None, None, :]\n",
    "    elif channels_axis == 1:\n",
    "        return x[:, :, None, None]\n",
    "    else:\n",
    "        raise ValueError(\"Slice axis should be in (1, 3), got {}.\".format(channels_axis))\n",
    "        \n",
    "def conv_block_2nd(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block_2nd(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def slice_tensor(x, start, stop, axis):\n",
    "    if axis == 3:\n",
    "        return x[:, :, :, start:stop]\n",
    "    elif axis == 1:\n",
    "        return x[:, start:stop, :, :]\n",
    "    else:\n",
    "        raise ValueError(\"Slice axis should be in (1, 3), got {}.\".format(axis))\n",
    "\n",
    "def GroupConv2D(filters,\n",
    "                kernel_size,\n",
    "                strides=(1, 1),\n",
    "                groups=32,\n",
    "                kernel_initializer='he_uniform',\n",
    "                use_bias=True,\n",
    "                activation='linear',\n",
    "                padding='valid',\n",
    "                **kwargs):\n",
    "    \"\"\"\n",
    "    Grouped Convolution Layer implemented as a Slice,\n",
    "    Conv2D and Concatenate layers. Split filters to groups, apply Conv2D and concatenate back.\n",
    "    Args:\n",
    "        filters: Integer, the dimensionality of the output space\n",
    "            (i.e. the number of output filters in the convolution).\n",
    "        kernel_size: An integer or tuple/list of a single integer,\n",
    "            specifying the length of the 1D convolution window.\n",
    "        strides: An integer or tuple/list of a single integer, specifying the stride\n",
    "            length of the convolution.\n",
    "        groups: Integer, number of groups to split input filters to.\n",
    "        kernel_initializer: Regularizer function applied to the kernel weights matrix.\n",
    "        use_bias: Boolean, whether the layer uses a bias vector.\n",
    "        activation: Activation function to use (see activations).\n",
    "            If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "        padding: one of \"valid\" or \"same\" (case-insensitive).\n",
    "    Input shape:\n",
    "        4D tensor with shape: (batch, rows, cols, channels) if data_format is \"channels_last\".\n",
    "    Output shape:\n",
    "        4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format is \"channels_last\".\n",
    "        rows and cols values might have changed due to padding.\n",
    "    \"\"\"\n",
    "\n",
    "    slice_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        inp_ch = int(backend.int_shape(input_tensor)[-1] // groups)  # input grouped channels\n",
    "        out_ch = int(filters // groups)  # output grouped channels\n",
    "\n",
    "        blocks = []\n",
    "        for c in range(groups):\n",
    "            slice_arguments = {\n",
    "                'start': c * inp_ch,\n",
    "                'stop': (c + 1) * inp_ch,\n",
    "                'axis': slice_axis,\n",
    "            }\n",
    "            x = Lambda(slice_tensor, arguments=slice_arguments)(input_tensor)\n",
    "            x = Conv2D(out_ch,\n",
    "                              kernel_size,\n",
    "                              strides=strides,\n",
    "                              kernel_initializer=kernel_initializer,\n",
    "                              use_bias=use_bias,\n",
    "                              activation=activation,\n",
    "                              padding=padding)(x)\n",
    "            blocks.append(x)\n",
    "\n",
    "        x = Concatenate(axis=slice_axis)(blocks)\n",
    "        return x\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05eaa2a-cae0-418b-b9b4-5dc879ebcaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNeXt(\n",
    "        model_params,\n",
    "        include_top=True,\n",
    "        input_tensor=None,\n",
    "        input_shape=None,\n",
    "        classes=1000,\n",
    "        weights='imagenet',\n",
    "        **kwargs):\n",
    "    \"\"\"Instantiates the ResNet, SEResNet architecture.\n",
    "    Optionally loads weights pre-trained on ImageNet.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "    Args:\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor\n",
    "            (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `channels_last` data format)\n",
    "            or `(3, 224, 224)` (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    Returns:\n",
    "        A Keras model instance.\n",
    "    Raises:\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape, name='data')\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    # get parameters for model layers\n",
    "    no_scale_bn_params = get_bn_params(scale=False)\n",
    "    bn_params = get_bn_params()\n",
    "    conv_params = get_conv_params()\n",
    "\n",
    "    # resnext bottom\n",
    "    x = BatchNormalization(name='bn_data', **no_scale_bn_params)(img_input)\n",
    "    x = ZeroPadding2D(padding=(3, 3))(x)\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), name='conv0', **conv_params)(x)\n",
    "    x = BatchNormalization(name='bn0', **bn_params)(x)\n",
    "    x = Activation('relu', name='relu0')(x)\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='valid', name='pooling0')(x)\n",
    "\n",
    "    # resnext body\n",
    "    init_filters = 128\n",
    "    for stage, rep in enumerate(model_params.repetitions):\n",
    "        for block in range(rep):\n",
    "\n",
    "            filters = init_filters * (2 ** stage)\n",
    "\n",
    "            # first block of first stage without strides because we have maxpooling before\n",
    "            if stage == 0 and block == 0:\n",
    "                x = conv_block(filters, stage, block, strides=(1, 1), **kwargs)(x)\n",
    "\n",
    "            elif block == 0:\n",
    "                x = conv_block(filters, stage, block, strides=(2, 2), **kwargs)(x)\n",
    "\n",
    "            else:\n",
    "                x = identity_block(filters, stage, block, **kwargs)(x)\n",
    "\n",
    "    # resnext top\n",
    "    if include_top:\n",
    "        x = GlobalAveragePooling2D(name='pool1')(x)\n",
    "        x = Dense(classes, name='fc1')(x)\n",
    "        x = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "    # Ensure that the model takes into account any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs, x)\n",
    "\n",
    "    if weights:\n",
    "        if type(weights) == str and os.path.exists(weights):\n",
    "            model.load_weights(weights)\n",
    "        else:\n",
    "            load_model_weights(model, model_params.model_name,\n",
    "                               weights, classes, include_top, **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426bae8-7541-4974-9364-c3a05098faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChannelSE(reduction=16, **kwargs):\n",
    "    \"\"\"\n",
    "    Squeeze and Excitation block, reimplementation inspired by\n",
    "        https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py\n",
    "    Args:\n",
    "        reduction: channels squeeze factor\n",
    "    \"\"\"\n",
    "    channels_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        # get number of channels/filters\n",
    "        channels = backend.int_shape(input_tensor)[channels_axis]\n",
    "\n",
    "        x = input_tensor\n",
    "\n",
    "        # squeeze and excitation block in PyTorch style with\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Lambda(expand_dims, arguments={'channels_axis': channels_axis})(x)\n",
    "        x = Conv2D(channels // reduction, (1, 1), kernel_initializer='he_uniform')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2D(channels, (1, 1), kernel_initializer='he_uniform')(x)\n",
    "        x = Activation('sigmoid')(x)\n",
    "\n",
    "        # apply attention\n",
    "        x = Multiply()([input_tensor, x])\n",
    "\n",
    "        return x\n",
    "\n",
    "    return layer\n",
    "\n",
    "def SEResNeXtBottleneck(filters, reduction=16, strides=1, groups=32, base_width=4, **kwargs):\n",
    "    bn_params = get_bn_params()\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        x = input_tensor\n",
    "        residual = input_tensor\n",
    "\n",
    "        width = (filters // 4) * base_width * groups // 64\n",
    "\n",
    "        # bottleneck\n",
    "        x = Conv2D(width, (1, 1), kernel_initializer='he_uniform', use_bias=False)(x)\n",
    "        x = BatchNormalization(**bn_params)(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        x = ZeroPadding2D(1)(x)\n",
    "        x = GroupConv2D(width, (3, 3), strides=strides, groups=groups,\n",
    "                        kernel_initializer='he_uniform', use_bias=False, **kwargs)(x)\n",
    "        x = BatchNormalization(**bn_params)(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        x = Conv2D(filters, (1, 1), kernel_initializer='he_uniform', use_bias=False)(x)\n",
    "        x = BatchNormalization(**bn_params)(x)\n",
    "\n",
    "        #  if number of filters or spatial dimensions changed\n",
    "        #  make same manipulations with residual connection\n",
    "        x_channels = get_num_channels(x)\n",
    "        r_channels = get_num_channels(residual)\n",
    "\n",
    "        if strides != 1 or x_channels != r_channels:\n",
    "            residual = Conv2D(x_channels, (1, 1), strides=strides,\n",
    "                                     kernel_initializer='he_uniform', use_bias=False)(residual)\n",
    "            residual = BatchNormalization(**bn_params)(residual)\n",
    "\n",
    "        # apply attention module\n",
    "        x = ChannelSE(reduction=reduction, **kwargs)(x)\n",
    "\n",
    "        # add residual connection\n",
    "        x = Add()([x, residual])\n",
    "\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318b900-476c-4d97-bb5d-5568af84e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SEResNext50(\n",
    "        model_params,\n",
    "        input_tensor=None,\n",
    "        input_shape=None,\n",
    "        include_top=True,\n",
    "        classes=1000,\n",
    "        weights='imagenet',\n",
    "        **kwargs\n",
    "):\n",
    "    \"\"\"Instantiates the ResNet, SEResNet architecture.\n",
    "    Optionally loads weights pre-trained on ImageNet.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "    Args:\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor\n",
    "            (i.e. output of `Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `channels_last` data format)\n",
    "            or `(3, 224, 224)` (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    Returns:\n",
    "        A Keras model instance.\n",
    "    Raises:\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    residual_block = model_params.residual_block\n",
    "    init_filters = model_params.init_filters\n",
    "    bn_params = get_bn_params()\n",
    "\n",
    "    # define input\n",
    "    if input_tensor is None:\n",
    "        input = Input(shape=input_shape, name='input')\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            input = input_tensor\n",
    "\n",
    "    x = input\n",
    "\n",
    "    if model_params.input_3x3:\n",
    "\n",
    "        x = ZeroPadding2D(1)(x)\n",
    "        x = Conv2D(init_filters, (3, 3), strides=2,\n",
    "                          use_bias=False, kernel_initializer='he_uniform')(x)\n",
    "        x = BatchNormalization(**bn_params)(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        x = ZeroPadding2D(1)(x)\n",
    "        x = Conv2D(init_filters, (3, 3), use_bias=False,\n",
    "                          kernel_initializer='he_uniform')(x)\n",
    "        x = BatchNormalization(**bn_params)(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        x = ZeroPadding2D(1)(x)\n",
    "        x = Conv2D(init_filters * 2, (3, 3), use_bias=False,\n",
    "                          kernel_initializer='he_uniform')(x)\n",
    "        x = BatchNormalization(**bn_params)(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "    else:\n",
    "        x = ZeroPadding2D(3)(x)\n",
    "        x = Conv2D(init_filters, (7, 7), strides=2, use_bias=False,\n",
    "                          kernel_initializer='he_uniform')(x)\n",
    "        x = BatchNormalization(**bn_params)(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "    x = ZeroPadding2D(1)(x)\n",
    "    x = MaxPooling2D((3, 3), strides=2)(x)\n",
    "\n",
    "    # body of resnet\n",
    "    filters = model_params.init_filters * 2\n",
    "    for i, stage in enumerate(model_params.repetitions):\n",
    "\n",
    "        # increase number of filters with each stage\n",
    "        filters *= 2\n",
    "\n",
    "        for j in range(stage):\n",
    "\n",
    "            # decrease spatial dimensions for each stage (except first, because we have maxpool before)\n",
    "            if i == 0 and j == 0:\n",
    "                x = residual_block(filters, reduction=model_params.reduction,\n",
    "                                   strides=1, groups=model_params.groups, is_first=True, **kwargs)(x)\n",
    "\n",
    "            elif i != 0 and j == 0:\n",
    "                x = residual_block(filters, reduction=model_params.reduction,\n",
    "                                   strides=2, groups=model_params.groups, **kwargs)(x)\n",
    "            else:\n",
    "                x = residual_block(filters, reduction=model_params.reduction,\n",
    "                                   strides=1, groups=model_params.groups, **kwargs)(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        if model_params.dropout is not None:\n",
    "            x = Dropout(model_params.dropout)(x)\n",
    "        x = Dense(classes)(x)\n",
    "        x = Activation('softmax', name='output')(x)\n",
    "\n",
    "    # Ensure that the model takes into account any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = input\n",
    "\n",
    "    model = Model(inputs, x, name=\"SEResNext50\")\n",
    "\n",
    "    if weights:\n",
    "        if type(weights) == str and os.path.exists(weights):\n",
    "            model.load_weights(weights)\n",
    "        else:\n",
    "            load_model_weights(model, model_params.model_name,\n",
    "                               weights, classes, include_top, **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16705e81-ec23-4332-a166-2ce927e41724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seresnext50_unet(input_shape):\n",
    "    inputs = Input(input_shape, name=\"input_1\")\n",
    "    \"\"\" Pre-trained ResNet50 Model \"\"\"\n",
    "    MODEL_PARS = ModelParams(\n",
    "        'seresnext50', repetitions=(3, 4, 6, 3), residual_block=SEResNeXtBottleneck,\n",
    "        groups=32, reduction=16, init_filters=64, input_3x3=False, dropout=None,\n",
    "    )\n",
    "    seresnext50 = SEResNext50(MODEL_PARS, weights=None, input_tensor=inputs)\n",
    "    \n",
    "    # seresnext50.summary()\n",
    "    # for idx, layer in enumerate(seresnext50.layers):\n",
    "    #     print(idx, layer.name, layer.output.type_spec.shape)\n",
    "        \n",
    "    \"\"\" Encoder \"\"\"\n",
    "    s1 = seresnext50.get_layer(index=0).output           ## (512 x 512)\n",
    "    s2 = seresnext50.get_layer(index=4).output        ## (256 x 256)\n",
    "    s3 = seresnext50.get_layer(index=257).output  ## (128 x 128)\n",
    "    s4 = seresnext50.get_layer(index=587).output  ## (64 x 64)\n",
    "    s5 = seresnext50.get_layer(index=1081).output  ## (32 x 32)\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    b1 = seresnext50.get_layer(index=1326).output  ## (16 x 16)\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    x = IMG_H\n",
    "    d1 = decoder_block(b1, s5, x)                     ## (32 x 32)\n",
    "    x = x/2\n",
    "    d2 = decoder_block(d1, s4, x)                     ## (64 x 64)\n",
    "    x = x/2\n",
    "    d3 = decoder_block(d2, s3, x)                     ## (128 x 128)\n",
    "    x = x/2\n",
    "    d4 = decoder_block(d3, s2, x)                      ## (256 x 256)\n",
    "    x = x/2\n",
    "    d5 = decoder_block(d4, s1, x)                      ## (512 x 512)\n",
    "\n",
    "\n",
    "    \"\"\" Output \"\"\"\n",
    "    outputs = Conv2D(IMG_C, 1, padding=\"same\", activation=\"tanh\")(d4)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"SEResNext50_U-Net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9aad2-6f9e-40b4-94ab-3c965a4a6f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create discriminator model\n",
    "def build_discriminator(inputs):\n",
    "    num_layers = 4\n",
    "    if IMG_H > 128:\n",
    "        num_layers = 5\n",
    "    f = [2**i for i in range(num_layers)]\n",
    "    x = inputs\n",
    "    features = []\n",
    "    for i in range(0, num_layers):\n",
    "        if i == 0:\n",
    "            x = tf.keras.layers.DepthwiseConv2D(kernel_size = (3, 3), strides=(2, 2), padding='same')(x)\n",
    "            x = tf.keras.layers.Conv2D(f[i] * IMG_H ,kernel_size = (1, 1),strides=(2,2), padding='same')(x)\n",
    "            # x = tf.keras.layers.BatchNormalization()(x)\n",
    "            x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        else:\n",
    "            x = tf.keras.layers.DepthwiseConv2D(kernel_size = (3, 3), strides=(2, 2), padding='same')(x)\n",
    "            x = tf.keras.layers.Conv2D(f[i] * IMG_H ,kernel_size = (1, 1),strides=(2,2), padding='same')(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "        # x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        \n",
    "        features.append(x)\n",
    "           \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    features.append(x)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs, outputs = [features, output])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9333e-3ce6-4477-b5f3-e0e377a3fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(g_model_inner, d_model_inner, g_filepath, d_filepath, test_ds):\n",
    "    class_names = [\"normal\", \"defect\"] # normal = 0, defect = 1\n",
    "    \n",
    "    g_model_inner.load_weights(g_filepath)\n",
    "    d_model_inner.load_weights(d_filepath)\n",
    "    \n",
    "        \n",
    "    scores_ano = []\n",
    "    real_label = []\n",
    "    rec_loss_list = []\n",
    "    feat_loss_list = []\n",
    "    ssim_loss_list = []\n",
    "    # counter = 0\n",
    "    \n",
    "    for images, labels in tqdm(test_ds, desc='testing stages'):\n",
    "        loss_rec, loss_feat = 0.0, 0.0\n",
    "        score = 0\n",
    "        \n",
    "        # counter += 1\n",
    "        '''for normal'''\n",
    "        # temp_score, loss_rec, loss_feat = calculate_a_score(g_model_inner, d_model_inner, images)\n",
    "        # score = temp_score.numpy()\n",
    "        \n",
    "        \n",
    "        '''for sliding images & Crop LR'''\n",
    "        for image in images:\n",
    "            r_score, r_rec_loss, r_feat_loss = calculate_a_score(g_model_inner, d_model_inner, image)\n",
    "            if r_score.numpy() > score or score == 0:\n",
    "                score = r_score.numpy()\n",
    "                loss_rec = r_rec_loss\n",
    "                loss_feat = r_feat_loss\n",
    "                \n",
    "            \n",
    "        scores_ano = np.append(scores_ano, score)\n",
    "        real_label = np.append(real_label, labels.numpy()[0])\n",
    "        \n",
    "        rec_loss_list = np.append(rec_loss_list, loss_rec)\n",
    "        feat_loss_list = np.append(feat_loss_list, loss_feat)\n",
    "        # if (counter % 100) == 0:\n",
    "        #     print(counter, \" tested.\")\n",
    "    ''' Scale scores vector between [0, 1]'''\n",
    "    scores_ano = (scores_ano - scores_ano.min())/(scores_ano.max()-scores_ano.min())\n",
    "    \n",
    "    auc_out, threshold = roc(real_label, scores_ano, name_model)\n",
    "    print(\"auc: \", auc_out)\n",
    "    print(\"threshold: \", threshold)\n",
    "    \n",
    "    # histogram distribution of anomaly scores\n",
    "    plot_anomaly_score(scores_ano, real_label, \"anomaly_score_dist\", name_model)\n",
    "    \n",
    "    scores_ano = (scores_ano > threshold).astype(int)\n",
    "    cm = tf.math.confusion_matrix(labels=real_label, predictions=scores_ano).numpy()\n",
    "    TP = cm[1][1]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[0][0]\n",
    "    print(cm)\n",
    "    print(\n",
    "            \"model saved. TP %d:, FP=%d, FN=%d, TN=%d\" % (TP, FP, FN, TN)\n",
    "    )\n",
    "    plot_confusion_matrix(cm, class_names, title=name_model)\n",
    "\n",
    "    diagonal_sum = cm.trace()\n",
    "    sum_of_all_elements = cm.sum()\n",
    "    arr_result = [\n",
    "        f\"Accuracy: {(diagonal_sum / sum_of_all_elements)}\",\n",
    "        f\"False Alarm Rate (FPR): {(FP/(FP+TN))}\", \n",
    "        f\"TNR: {(TN/(FP+TN))}\", \n",
    "        f\"Precision Score (PPV): {(TP/(TP+FP))}\", \n",
    "        f\"Recall Score (TPR): {(TP/(TP+FN))}\", \n",
    "        f\"NPV: {(TN/(FN+TN))}\", \n",
    "        f\"F1-Score: {(f1_score(real_label, scores_ano))}\", \n",
    "        f\"Training Duration: {TRAINING_DURATION}\"\n",
    "        f\"Testing Duration: {TESTING_DURATION}\"\n",
    "    ]\n",
    "    print(\"\\n\".join(arr_result))\n",
    "    \n",
    "    # print(\"Accuracy: \", diagonal_sum / sum_of_all_elements)\n",
    "    # print(\"False Alarm Rate (FPR): \", FP/(FP+TN))\n",
    "    # print(\"Leakage Rat (FNR): \", FN/(FN+TP))\n",
    "    # print(\"TNR: \", TN/(FP+TN))\n",
    "    # print(\"precision_score: \", TP/(TP+FP))\n",
    "    # print(\"recall_score: \", TP/(TP+FN))\n",
    "    # print(\"NPV: \", TN/(FN+TN))\n",
    "    # print(\"F1-Score: \", f1_score(real_label, scores_ano))\n",
    "    \n",
    "    write_result(arr_result, name_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4daa4c-f180-4134-ab4d-372cadd211b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMG_H, IMG_W, IMG_C)\n",
    "# set input \n",
    "inputs = tf.keras.layers.Input(input_shape, name=\"input_1\")\n",
    "# inputs_disc = tf.keras.layers.Input((IMG_H, IMG_W, 1), name=\"input_1\")\n",
    "\n",
    "g_model = build_seresnext50_unet(input_shape)\n",
    "d_model = build_discriminator(inputs)\n",
    "# grayscale_converter = tf.keras.layers.Lambda(lambda x: tf.image.rgb_to_grayscale(x))\n",
    "d_model.compile()\n",
    "g_model.compile()\n",
    "g_optimizer = GCAdam(learning_rate=learning_rate, beta_1=0.5, beta_2=0.999)\n",
    "# g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "d_optimizer = GCAdam(learning_rate=learning_rate, beta_1=0.5, beta_2=0.999)\n",
    "# d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920087d-5202-4ef1-8ad8-13e33e71a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADV_REG_RATE_LF = 1\n",
    "REC_REG_RATE_LF = 50\n",
    "SSIM_REG_RATE_LF = 10\n",
    "FEAT_REG_RATE_LF = 1\n",
    "\n",
    "gen_loss_list = []\n",
    "disc_loss_list = []\n",
    "iter_list = []\n",
    "auc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b491a4-8a36-4df0-ac01-990231e49246",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # tf.print(\"Images: \", images)\n",
    "        reconstructed_images = g_model(real_images, training=True)\n",
    "        \n",
    "        # real_images = grayscale_converter(real_images)\n",
    "        feature_real, label_real = d_model(real_images, training=True)\n",
    "        # print(generated_images.shape)\n",
    "        feature_fake, label_fake = d_model(reconstructed_images, training=True)\n",
    "\n",
    "        discriminator_fake_average_out = tf.math.reduce_mean(label_fake, axis=0)\n",
    "        discriminator_real_average_out = tf.math.reduce_mean(label_real, axis=0)\n",
    "        real_fake_ra_out = label_real - discriminator_fake_average_out\n",
    "        fake_real_ra_out = label_fake - discriminator_real_average_out\n",
    "        epsilon = 0.000001\n",
    "        \n",
    "        # Loss 1: \n",
    "        # use relativistic average loss\n",
    "        loss_gen_ra = -( \n",
    "            tf.math.reduce_mean( \n",
    "                tf.math.log( \n",
    "                    tf.math.sigmoid(fake_real_ra_out) + epsilon), axis=0 \n",
    "            ) + tf.math.reduce_mean( \n",
    "                tf.math.log(1-tf.math.sigmoid(real_fake_ra_out) + epsilon), axis=0 \n",
    "            ) \n",
    "        )\n",
    "\n",
    "        loss_disc_ra = -( \n",
    "            tf.math.reduce_mean( \n",
    "                tf.math.log(\n",
    "                    tf.math.sigmoid(real_fake_ra_out) + epsilon), axis=0 \n",
    "            ) + tf.math.reduce_mean( \n",
    "                tf.math.log(1-tf.math.sigmoid(fake_real_ra_out) + epsilon), axis=0 \n",
    "            ) \n",
    "        )\n",
    "\n",
    "        # Loss 2: RECONSTRUCTION loss (L1)\n",
    "        loss_rec = mae(real_images, reconstructed_images)\n",
    "\n",
    "        # Loss 3: SSIM Loss\n",
    "        loss_ssim =  ssim(real_images, reconstructed_images)\n",
    "\n",
    "        # Loss 4: FEATURE Loss\n",
    "        # loss_feat = mse(feature_real, feature_fake)\n",
    "        loss_feat = multimse(feature_real, feature_fake, FEAT_REG_RATE_LF)\n",
    "\n",
    "        gen_loss = tf.reduce_mean( \n",
    "            (loss_gen_ra * ADV_REG_RATE_LF) \n",
    "            + (loss_rec * REC_REG_RATE_LF) \n",
    "            + (loss_ssim * SSIM_REG_RATE_LF) \n",
    "            + (loss_feat) \n",
    "        )\n",
    "\n",
    "        disc_loss = tf.reduce_mean( (loss_disc_ra * ADV_REG_RATE_LF) + (loss_feat * FEAT_REG_RATE_LF) )\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, d_model.trainable_variables)\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, g_model.trainable_variables)\n",
    "\n",
    "    d_optimizer.apply_gradients(zip(gradients_of_discriminator, d_model.trainable_variables))\n",
    "    g_optimizer.apply_gradients(zip(gradients_of_generator, g_model.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826985e9-9daa-4833-99d2-7c5fd97345b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    print(\"Start Trainning. \", name_model)\n",
    "    best_auc = 0.84\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    for meta_iter in tqdm(range(meta_iters), desc=f'training process'):\n",
    "        frac_done = meta_iter / meta_iters\n",
    "        cur_meta_step_size = (1 - frac_done) * meta_step_size\n",
    "        # Temporarily save the weights from the model.\n",
    "        d_old_vars = d_model.get_weights()\n",
    "        g_old_vars = g_model.get_weights()\n",
    "        # Get a sample from the full dataset.\n",
    "        mini_dataset = train_dataset.get_mini_dataset(\n",
    "            inner_batch_size, inner_iters, train_shots, classes\n",
    "        )\n",
    "        gen_loss_out = 0.0\n",
    "        disc_loss_out = 0.0\n",
    "        \n",
    "        # print(\"meta_iter: \", meta_iter)\n",
    "        for images, _ in mini_dataset:\n",
    "            g_loss, d_loss = train_step(images)\n",
    "            gen_loss_out = g_loss\n",
    "            disc_loss_out = d_loss\n",
    "            \n",
    "        d_new_vars = d_model.get_weights()\n",
    "        g_new_vars = g_model.get_weights()\n",
    "\n",
    "        # Perform SGD for the meta step.\n",
    "        for var in range(len(d_new_vars)):\n",
    "            d_new_vars[var] = d_old_vars[var] + (\n",
    "                (d_new_vars[var] - d_old_vars[var]) * cur_meta_step_size\n",
    "            )\n",
    "\n",
    "        for var in range(len(g_new_vars)):\n",
    "            g_new_vars[var] = g_old_vars[var] + (\n",
    "                (g_new_vars[var] - g_old_vars[var]) * cur_meta_step_size\n",
    "            )\n",
    "\n",
    "        # After the meta-learning step, reload the newly-trained weights into the model.\n",
    "        g_model.set_weights(g_new_vars)\n",
    "        d_model.set_weights(d_new_vars)\n",
    "        \n",
    "        # Evaluation loop\n",
    "        meta_iter = meta_iter + 1\n",
    "        if meta_iter % 100 == 0:\n",
    "            eval_g_model = g_model\n",
    "            eval_d_model = d_model\n",
    "            \n",
    "            iter_list = np.append(iter_list, meta_iter)\n",
    "            gen_loss_list = np.append(gen_loss_list, gen_loss_out)\n",
    "            disc_loss_list = np.append(disc_loss_list, disc_loss_out)\n",
    "\n",
    "            scores_ano = []\n",
    "            real_label = []\n",
    "            # counter = 0\n",
    "           \n",
    "            for images, labels in tqdm(eval_ds, desc=f'evalution stage at {meta_iter} batch'):\n",
    "\n",
    "                loss_rec, loss_feat = 0.0, 0.0\n",
    "                score = 0\n",
    "                # counter += 1\n",
    "                \n",
    "                '''for normal'''\n",
    "                # temp_score, loss_rec, loss_feat = calculate_a_score(eval_g_model, eval_d_model, images)\n",
    "                # score = temp_score.numpy()\n",
    "\n",
    "                '''for Sliding Images & LR Crop'''\n",
    "                for image in images:\n",
    "                    r_score, r_rec_loss, r_feat_loss = calculate_a_score(eval_g_model, eval_d_model, image)\n",
    "                    if r_score.numpy() > score or score == 0:\n",
    "                        score = r_score.numpy()\n",
    "                        loss_rec = r_rec_loss\n",
    "                        loss_feat = r_feat_loss\n",
    "                    \n",
    "                scores_ano = np.append(scores_ano, score)\n",
    "                real_label = np.append(real_label, labels.numpy()[0])\n",
    "                # if (counter % 100) == 0:\n",
    "                #     print(counter, \" tested.\")\n",
    "            # print(\"scores_ano:\", scores_ano)\n",
    "            '''Scale scores vector between [0, 1]'''\n",
    "            scores_ano = (scores_ano - scores_ano.min())/(scores_ano.max()-scores_ano.min())\n",
    "            # print(\"real_label:\", real_label)\n",
    "            # print(\"scores_ano:\", scores_ano)\n",
    "            auc_out, threshold = roc(real_label, scores_ano, name_model)\n",
    "            auc_list = np.append(auc_list, auc_out)\n",
    "            scores_ano = (scores_ano > threshold).astype(int)\n",
    "            cm = tf.math.confusion_matrix(labels=real_label, predictions=scores_ano).numpy()\n",
    "            TP = cm[1][1]\n",
    "            FP = cm[0][1]\n",
    "            FN = cm[1][0]\n",
    "            TN = cm[0][0]\n",
    "            # print(cm)\n",
    "            print(\n",
    "                f\"model saved. batch {meta_iter}:, AUC={auc_out:.3f}, TP={TP}, TN={TN}, FP={FP}, FN={FN}, Gen Loss={gen_loss_out:.5f}, Disc Loss={disc_loss_out:.5f}\" \n",
    "            )\n",
    "            \n",
    "            if auc_out >= best_auc:\n",
    "                print(\n",
    "                    f\"the best model saved. at batch {meta_iter}: with AUC={auc_out:.3f}\"\n",
    "                )\n",
    "                \n",
    "                best_g_model_path = g_model_path.replace(\".h5\", f\"_best_{meta_iter}_{auc_out:.2f}.h5\")\n",
    "                best_d_model_path = d_model_path.replace(\".h5\", f\"_best_{meta_iter}_{auc_out:.2f}.h5\")\n",
    "                g_model.save(best_g_model_path)\n",
    "                d_model.save(best_d_model_path)\n",
    "                best_auc = auc_out\n",
    "                \n",
    "            # save model's weights\n",
    "            g_model.save(g_model_path)\n",
    "            d_model.save(d_model_path)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    TRAINING_DURATION = end_time - start_time\n",
    "    print(f'Duration of Training: {end_time - start_time}')\n",
    "    \"\"\"\n",
    "    Train Ends\n",
    "    \"\"\"\n",
    "    plot_epoch_result(iter_list, gen_loss_list, \"Generator_Loss\", name_model, \"g\")\n",
    "    plot_epoch_result(iter_list, disc_loss_list, \"Discriminator_Loss\", name_model, \"r\")\n",
    "    plot_epoch_result(iter_list, auc_list, \"AUC\", name_model, \"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1b622-6b6f-430a-9bac-1c4e26e2a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(test_data_path, training=False, limit=LIMIT_TEST_IMAGES)\n",
    "\n",
    "start_time = datetime.now()\n",
    "testing(g_model, d_model, g_model_path, d_model_path, test_dataset.get_dataset(1))\n",
    "end_time = datetime.now()\n",
    "TESTING_DURATION = end_time - start_time\n",
    "print(f'Duration of Testing: {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0adf8-c786-4836-9aa0-df8c2781f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_gen_disc(name_model, g_model, d_model, g_model_path, d_model_path, test_data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
